import { PaperMeta } from "@/components/content/PaperMeta";

<PaperMeta
  title="Auto-Encoding Variational Bayes"
  titleKo="변분 오토인코더"
  authors={["Diederik P. Kingma", "Max Welling"]}
  year={2013}
  venue="ICLR 2014"
  venueType="iclr"
  arxivUrl="https://arxiv.org/abs/1312.6114"
  citations="25,000+"
/>

## 한줄 요약

**변분 추론(Variational Inference)** 과 **신경망 기반 인코더-디코더**를 결합하여, 복잡한 확률적 생성 모델을 효율적으로 학습하고 샘플링할 수 있는 프레임워크를 제안한 논문.

## 배경 & 동기

확률적 생성 모델의 핵심은 데이터의 잠재 변수(latent variable) 모델 $p_\theta(x|z)$를 학습하는 것이다. 그러나:

- **사후 확률 $p_\theta(z|x)$ 계산 불가**: $p_\theta(z|x) = p_\theta(x|z)p(z)/p_\theta(x)$에서 증거(evidence) $p_\theta(x) = \int p_\theta(x|z)p(z)dz$가 계산 불가능(intractable)
- **EM 알고리즘**: E-step에서 사후 확률이 필요하므로 적용 어려움
- **MCMC 기반 방법**: 수렴이 느리고 대규모 데이터에 비효율적
- 대규모 데이터셋에서 효율적으로 학습하면서, 의미 있는 잠재 공간을 형성하는 방법이 필요

<KeyIdea title="변분 오토인코더 (Variational Auto-Encoder)">
VAE의 핵심 아이디어는 두 가지다:

1. **인식 모델(Recognition Model)**: 계산 불가능한 사후 확률 $p_\theta(z|x)$를 신경망 $q_\phi(z|x)$로 근사. 이 인코더가 데이터를 잠재 공간으로 매핑
2. **Reparameterization Trick**: $z \sim q_\phi(z|x)$에서의 샘플링을 미분 가능하게 변환. $z = \mu + \sigma \odot \epsilon$, $\epsilon \sim \mathcal{N}(0, I)$로 표현하여 역전파 가능

이를 통해 **인코더**(데이터 → 잠재 변수)와 **디코더**(잠재 변수 → 데이터)를 동시에 end-to-end로 학습할 수 있다.
</KeyIdea>

<Formula title="Evidence Lower Bound (ELBO)">
$$\log p_\theta(x) \geq \mathcal{L}(\theta, \phi; x) = -D_{KL}(q_\phi(z|x) \| p(z)) + \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$$

여기서:
- $D_{KL}(q_\phi(z|x) \| p(z))$: **정규화 항** — 인코더 분포를 사전 분포(보통 $\mathcal{N}(0, I)$)에 가깝게 유지
- $\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$: **복원 항** — 잠재 변수로부터 원래 데이터를 잘 복원
- 이 ELBO를 최대화하는 것이 $\log p_\theta(x)$를 최대화하는 것의 하한(lower bound)
</Formula>

<Formula title="Reparameterization Trick">
인코더 $q_\phi(z|x) = \mathcal{N}(\mu, \sigma^2 I)$에서 직접 샘플링하면 역전파 불가. 대신:

$$z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

여기서:
- $\mu_\phi(x)$, $\sigma_\phi(x)$: 인코더 신경망의 출력
- $\epsilon$: 표준 정규분포에서 샘플링 (파라미터와 무관)
- $\odot$: 원소별 곱

이 트릭으로 샘플링 과정을 결정적 변환으로 바꿔 gradient가 $\phi$로 흐를 수 있게 한다.
</Formula>

<Formula title="가우시안 인코더의 KL Divergence (닫힌 해)">
$$D_{KL}(q_\phi(z|x) \| p(z)) = -\frac{1}{2} \sum_{j=1}^{J} \left(1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2\right)$$

여기서 $J$는 잠재 변수의 차원. 가우시안 가정 하에서 KL divergence를 해석적으로 계산 가능.
</Formula>

## 실험 결과

| 데이터셋 | 모델 | Marginal Likelihood (NLL) |
|---|---|---|
| MNIST | Wake-Sleep | ~86.35 |
| MNIST | **VAE** | **~86.76** |
| Frey Face | Wake-Sleep | ~1361 |
| Frey Face | **VAE** | **~1324** |

- MNIST, Frey Face 데이터셋에서 기존 변분 방법(Wake-Sleep 등) 대비 동등 또는 우수한 성능
- 잠재 공간에서의 **연속적이고 의미 있는 보간(interpolation)** 이 가능함을 시각적으로 시연
- 학습된 잠재 공간이 매끄러운(smooth) 매니폴드를 형성 — 숫자 스타일의 연속적 변환 가능

<Impact>
**확률적 생성 모델의 핵심 프레임워크를 확립한 논문.**

- GAN과 함께 현대 생성 모델의 **양대 축**을 형성 (VAE: 확률론적 / GAN: 적대적)
- **Reparameterization Trick**은 확률적 계산 그래프 학습의 표준 기법이 됨
- VQ-VAE, $\beta$-VAE, CVAE 등 수많은 후속 변형 모델의 기반
- **Diffusion Model**의 이론적 뿌리: DDPM의 변분 하한(VLB) 목적 함수가 VAE의 ELBO에서 직접 파생
- Stable Diffusion의 Latent Diffusion Model은 VAE 인코더로 잠재 공간을 만든 후 그 위에서 확산
- 약물 발견, 분자 생성, 음악 생성, 이상 탐지 등 폭넓은 응용
- 공저자 Kingma는 Adam optimizer(인용 20만+)의 저자이기도 함
- 인용 수 2만 5천회 이상
</Impact>

<FoundationFieldLinks paperId="vae" />
