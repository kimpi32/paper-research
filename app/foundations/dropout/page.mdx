import { PaperMeta } from "@/components/content/PaperMeta";

<PaperMeta
  title="Dropout: A Simple Way to Prevent Neural Networks from Overfitting"
  titleKo="드롭아웃: 신경망의 과적합을 방지하는 간단한 방법"
  authors={["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"]}
  year={2014}
  venue="JMLR"
  venueType="jmlr"
  conferenceUrl="https://jmlr.org/papers/v15/srivastava14a.html"
  citations="50,000+"
/>

## 한줄 요약

학습 시 뉴런을 **랜덤으로 비활성화**하여 과적합을 방지하는 정규화 기법. 극히 단순하면서도 강력한 효과.

## 배경 & 동기

대규모 신경망의 과적합이 핵심 문제:
- 파라미터 수가 학습 데이터보다 많으면 훈련 데이터를 외워버림
- 기존 정규화: L1/L2, 조기 종료, 데이터 증강 등
- 앙상블이 효과적이지만, 대규모 모델 여러 개를 학습하는 비용이 큼

<KeyIdea title="학습 중 뉴런의 랜덤 비활성화">
각 학습 스텝에서 뉴런을 확률 $p$로 **비활성화(drop)**. 이는:

- **암묵적 앙상블**: 매 스텝마다 다른 서브네트워크를 학습하는 효과
- **공동 적응 방지**: 뉴런들이 서로에게 의존하지 않고 독립적으로 유용한 특징 학습
- **추론 시**: 모든 뉴런 활성화, 가중치에 $p$를 곱하여 스케일 보정
</KeyIdea>

<Formula title="Dropout 수식">
학습 시:
$$\tilde{h}_i = r_i \cdot h_i, \quad r_i \sim \text{Bernoulli}(p)$$

추론 시:
$$h_i^{\text{test}} = p \cdot h_i$$

또는 Inverted Dropout (실무에서 더 흔함):
$$\tilde{h}_i = \frac{r_i}{p} \cdot h_i \quad \text{(학습 시 스케일링, 추론 시 변경 없음)}$$
</Formula>

## 실험 결과

다양한 벤치마크에서 일관된 개선:

| 데이터셋 | Without Dropout | With Dropout |
|---|---|---|
| MNIST | 1.60% | **1.25%** |
| SVHN | 2.80% | **2.40%** |
| CIFAR-10 | 16.6% | **15.6%** |
| ImageNet | 40.7% (Top-1) | **36.4%** |

- 거의 모든 경우에서 개선, 특히 **큰 네트워크**에서 효과가 큼
- 보통 $p = 0.5$ (은닉층) 또는 $p = 0.8$ (입력층)이 최적

<Impact>
**딥러닝의 표준 정규화 기법.**

- 거의 모든 신경망 아키텍처에서 기본으로 사용
- 후속: **DropConnect**, **Spatial Dropout**, **DropBlock**, **Stochastic Depth**
- Batch Normalization(2015)과 함께 현대 딥러닝의 기본 레시피
- Transformer에서는 Attention dropout, Residual dropout 등으로 변형 사용
- 인용 수 5만회 이상
</Impact>
