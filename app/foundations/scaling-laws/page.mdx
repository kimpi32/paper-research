import { PaperMeta } from "@/components/content/PaperMeta";

<PaperMeta
  title="Scaling Laws for Neural Language Models"
  titleKo="신경 언어 모델의 스케일링 법칙"
  authors={["Jared Kaplan", "Sam McCandlish", "Tom Henighan", "et al."]}
  year={2020}
  venue="arXiv"
  venueType="arxiv"
  arxivUrl="https://arxiv.org/abs/2001.08361"
  citations="5,000+"
/>

## 한줄 요약

언어 모델의 성능이 모델 크기, 데이터 크기, 계산량에 대해 **거듭제곱 법칙(power law)**을 따른다는 것을 실증한 논문. LLM 스케일링의 이론적 근거.

## 배경 & 동기

GPT-2(2019)가 모델을 키우면 성능이 좋아진다는 직관을 보여줬지만:
- **얼마나** 키워야 하는지, **무엇을** 키워야 하는지 체계적 분석 부재
- 모델 크기 vs 데이터 크기 vs 학습 시간 중 어디에 자원을 투자?
- 대규모 학습의 효율적 자원 배분 전략 필요

<KeyIdea title="Power Law Scaling">
세 변수에 대해 손실이 거듭제곱 법칙을 따름:

1. **모델 크기 $N$** (비임베딩 파라미터 수): $L(N) \propto N^{-0.076}$
2. **데이터 크기 $D$** (토큰 수): $L(D) \propto D^{-0.095}$
3. **계산량 $C$** (FLOPs): $L(C) \propto C^{-0.050}$

핵심 발견: **아키텍처 세부사항**(깊이 vs 너비, attention head 수 등)은 상대적으로 덜 중요하고, **총 파라미터 수**가 성능을 결정.
</KeyIdea>

<Formula title="스케일링 법칙">
$$L(N) = \left(\frac{N_c}{N}\right)^{\alpha_N}, \quad \alpha_N \approx 0.076$$
$$L(D) = \left(\frac{D_c}{D}\right)^{\alpha_D}, \quad \alpha_D \approx 0.095$$
$$L(C) = \left(\frac{C_c}{C}\right)^{\alpha_C}, \quad \alpha_C \approx 0.050$$

결합 법칙:
$$L(N, D) = \left[\left(\frac{N_c}{N}\right)^{\alpha_N / \alpha_D} + \frac{D_c}{D}\right]^{\alpha_D}$$
</Formula>

## 핵심 발견

- 성능은 10배 계산 증가 시 약 **일정 비율**로 개선 (수확 체감이 매우 느림)
- **고정 계산 예산**에서는 큰 모델을 적게 학습하는 것이 최적
- 모델 크기와 데이터 크기를 **동시에** 늘려야 효율적
- Transformer의 깊이/너비 비율은 큰 영향 없음

<Impact>
**LLM 스케일링 경쟁의 이론적 근거.**

- GPT-3(2020), PaLM(2022), GPT-4(2023) 등 대규모 모델 개발의 직접적 동기
- **Chinchilla 논문**(Hoffmann et al., 2022)이 이를 수정: 데이터가 더 중요하다는 새로운 스케일링 법칙 제시
- AI 기업의 **컴퓨팅 투자 전략**에 직접적 영향
- "모델을 키우면 새로운 능력이 나타난다"는 경험적 관찰의 정량적 뒷받침
- 인용 수 5,000+
</Impact>
