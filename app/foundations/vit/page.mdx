import { PaperMeta } from "@/components/content/PaperMeta";

<PaperMeta
  title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  titleKo="이미지는 16x16 단어의 가치가 있다: 대규모 이미지 인식을 위한 트랜스포머"
  authors={["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "et al."]}
  year={2020}
  venue="ICLR 2021"
  venueType="iclr"
  arxivUrl="https://arxiv.org/abs/2010.11929"
  citations="30,000+"
/>

## 한줄 요약

이미지를 **패치 시퀀스**로 변환하여 **순수 Transformer**로 이미지 분류를 수행. CNN 없이도 대규모 데이터에서 SOTA를 달성하며, 비전에서의 Transformer 시대를 열었다.

## 배경 & 동기

Transformer가 NLP를 지배하고 있었으나, 컴퓨터 비전은 여전히 CNN 기반:
- Attention을 CNN에 보조적으로 추가하는 시도는 있었음
- **순수 Transformer**로 이미지를 처리하려는 시도는 성공 사례 부재
- 핵심 문제: 이미지의 픽셀 수가 많아 self-attention의 $O(n^2)$ 비용이 비현실적

<KeyIdea title="이미지를 패치 시퀀스로">
이미지를 고정 크기 패치로 잘라 **토큰 시퀀스**처럼 취급:

1. 이미지 (H×W×C)를 P×P 크기의 패치로 분할 → $N = HW/P^2$개 패치
2. 각 패치를 1D 벡터로 펼친 후 선형 프로젝션 → **패치 임베딩**
3. [CLS] 토큰 + 위치 임베딩 추가
4. 표준 Transformer Encoder에 입력
5. [CLS] 토큰의 출력으로 분류

CNN의 **귀납적 편향**(지역성, 이동 등변성) 없이, 순수하게 데이터에서 학습.
</KeyIdea>

<Formula title="패치 임베딩">
이미지 $x \in \mathbb{R}^{H \times W \times C}$를 $N$개 패치로 분할:

$$z_0 = [x_{\text{class}}; \; x_p^1 E; \; x_p^2 E; \; \cdots; \; x_p^N E] + E_{\text{pos}}$$

여기서:
- $x_p^i \in \mathbb{R}^{P^2 \cdot C}$: $i$번째 패치 (펼친 것)
- $E \in \mathbb{R}^{(P^2 \cdot C) \times D}$: 패치 임베딩 프로젝션
- $E_{\text{pos}} \in \mathbb{R}^{(N+1) \times D}$: 학습 가능한 위치 임베딩
- $x_{\text{class}}$: [CLS] 토큰
</Formula>

## 모델 구조

| 모델 | 레이어 | Hidden | Heads | 파라미터 |
|---|---|---|---|---|
| ViT-Base/16 | 12 | 768 | 12 | 86M |
| ViT-Large/16 | 24 | 1024 | 16 | 307M |
| ViT-Huge/14 | 32 | 1280 | 16 | 632M |

"/16"은 패치 크기 16×16을 의미. 224×224 이미지 → 196개 패치.

## 실험 결과

| 모델 | ImageNet Top-1 Acc | 사전학습 데이터 |
|---|---|---|
| BiT-L (ResNet) | 87.5% | JFT-300M |
| **ViT-H/14** | **88.6%** | JFT-300M |
| ViT-B/16 | 77.9% | ImageNet-1K |

핵심 발견:
- **대규모 데이터**(JFT-300M)로 사전학습 시 CNN을 능가
- ImageNet만으로는 CNN 대비 열세 → 귀납적 편향의 부재 때문
- 데이터 스케일이 귀납적 편향을 대체할 수 있음

<Impact>
**컴퓨터 비전의 CNN → Transformer 전환을 촉발한 논문.**

- **DeiT, Swin Transformer, BEiT** 등 비전 Transformer 후속 연구 폭발
- Swin Transformer: 계층적 구조로 검출/분할에도 적용 → CNN을 전면 대체
- **CLIP**: ViT + 텍스트 인코더로 비전-언어 모델
- 현재 대부분의 SOTA 비전 모델이 Transformer 기반
- "NLP와 비전의 아키텍처 통합"이라는 방향 제시
- 인용 수 3만회 이상
</Impact>
