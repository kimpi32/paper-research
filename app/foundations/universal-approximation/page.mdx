import { PaperMeta } from "@/components/content/PaperMeta";

<PaperMeta
  title="Multilayer feedforward networks are universal approximators"
  titleKo="다층 순전파 네트워크는 보편적 근사기이다"
  authors={["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"]}
  year={1989}
  venue="Neural Networks"
  venueType="other"
  conferenceUrl="https://doi.org/10.1016/0893-6080(89)90020-8"
  citations="30,000+"
/>

## 한줄 요약

은닉층이 하나인 순전파 신경망이 **임의의 연속 함수를 원하는 정밀도로 근사**할 수 있음을 수학적으로 증명한 논문.

## 배경 & 동기

1980년대 후반, 역전파 알고리즘의 등장으로 다층 신경망 학습이 가능해졌지만 근본적인 의문이 남아 있었다:

- 신경망이 **어떤 종류의 함수**를 표현할 수 있는가?
- 은닉층의 뉴런을 충분히 늘리면 **어떤 함수든** 근사 가능한가?
- 신경망의 **표현력(expressiveness)**에 이론적 한계가 있는가?

<KeyIdea title="보편적 근사 정리">
하나의 은닉층을 가진 순전파 네트워크가 비상수, 유계, 단조 증가하는 연속 활성화 함수(예: sigmoid)를 사용할 때, 충분한 수의 은닉 뉴런이 있으면 $\mathbb{R}^n$의 임의의 컴팩트 부분집합 위에서 임의의 연속 함수를 원하는 정밀도로 근사할 수 있다.

이는 **존재 정리**이다 — 그러한 근사가 "가능하다"는 것이지, 효율적으로 찾을 수 있다거나 필요한 뉴런 수가 적다는 것은 아니다.
</KeyIdea>

<Formula title="정리의 형식적 서술">
$\sigma$가 비상수, 유계, 단조 증가하는 연속 함수라 하자. $I_n = [0,1]^n$이라 하자. 그러면 $I_n$ 위의 임의의 연속 함수 $f$와 임의의 $\varepsilon > 0$에 대해,

$$G(x) = \sum_{j=1}^{N} \alpha_j \, \sigma\!\left(\sum_{i=1}^{n} w_{ij} x_i + b_j\right)$$

형태의 함수 $G$가 존재하여:

$$\sup_{x \in I_n} |f(x) - G(x)| < \varepsilon$$

즉, 단일 은닉층 네트워크 $G$로 $f$를 $\varepsilon$-정밀도로 균일하게 근사할 수 있다.
</Formula>

## 의미와 한계

### 정리가 말하는 것
- 신경망은 이론적으로 **어떤 연속 함수든** 표현할 수 있다
- 표현력의 **원천**은 네트워크의 깊이가 아니라 **폭(너비)**

### 정리가 말하지 않는 것
- 필요한 뉴런 수가 **얼마인지** (지수적으로 많을 수 있음)
- 경사 하강법으로 **찾을 수 있는지** (학습 가능성은 별개)
- **깊은** 네트워크가 **얕은** 네트워크보다 효율적인지

<Impact>
**신경망의 이론적 정당성을 확립한 논문.**

- 신경망이 "충분히 강력한 함수 근사기"라는 **이론적 보증**을 제공
- 이후 딥러닝 붐의 이론적 토대 — "원리적으로 가능하다"는 확신
- 후속 연구: 깊이의 효율성(Eldan & Shamir 2016), ReLU 네트워크의 근사 이론, Neural Tangent Kernel 등
- 현대적 관점: 존재 정리를 넘어 **깊이가 왜 도움이 되는지**, **어떤 구조가 효율적인지**에 초점이 이동
- 인용 수 3만회 이상
</Impact>
