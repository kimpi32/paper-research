import { PaperMeta } from "@/components/content/PaperMeta";

<PaperMeta
  title="Improving Language Understanding by Generative Pre-Training"
  titleKo="생성적 사전 학습을 통한 언어 이해 향상"
  authors={["Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever"]}
  year={2018}
  venue="OpenAI"
  venueType="other"
  conferenceUrl="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"
  citations="10,000+"
/>

## 한줄 요약

**비지도 사전 학습(Generative Pre-Training)** + **지도 미세 조정(Supervised Fine-Tuning)** 의 2단계 접근으로, 레이블 없는 대량의 텍스트로부터 범용 언어 표현을 학습할 수 있음을 보인 논문. GPT 시리즈의 출발점.

## 배경 & 동기

2018년 당시 NLP의 주요 과제:

- **레이블 데이터 부족**: 대부분의 NLP 태스크는 레이블 데이터가 적어 모델 학습이 어려움
- **태스크별 아키텍처**: 분류, QA, 추론 등 각 태스크마다 별도 모델을 설계해야 함
- **Word2Vec, GloVe**: 단어 수준 임베딩은 유용하지만 문맥을 반영하지 못함
- **ELMo**(2018): 문맥 임베딩을 제공하지만 LSTM 기반이고, 특징 추출기로만 사용
- 비지도 텍스트로부터 **범용적 언어 이해 능력**을 학습하고 다양한 태스크에 전이하는 방법이 필요

<KeyIdea title="생성적 사전 학습 + 판별적 미세 조정">
GPT-1은 2단계 학습 전략을 제안한다:

1. **사전 학습 (Unsupervised Pre-Training)**: 대규모 비지도 코퍼스에서 **다음 토큰 예측**(언어 모델링)으로 Transformer Decoder를 학습. 문법, 의미, 세계 지식을 암묵적으로 습득
2. **미세 조정 (Supervised Fine-Tuning)**: 사전 학습된 모델에 태스크별 입력 형식을 적용하고 소량의 레이블 데이터로 미세 조정. 모델 구조 변경 최소화

핵심 통찰: Transformer Decoder의 **단방향 언어 모델링**이 충분히 강력한 사전 학습 목적 함수가 될 수 있다.
</KeyIdea>

<Formula title="사전 학습: 언어 모델링 목적 함수">
$$L_1(\mathcal{U}) = \sum_i \log P(u_i \mid u_{i-k}, ..., u_{i-1}; \Theta)$$

여기서:
- $\mathcal{U} = \{u_1, ..., u_n\}$: 비지도 텍스트 코퍼스의 토큰 시퀀스
- $k$: 컨텍스트 윈도우 크기
- $\Theta$: 모델 파라미터
- 각 토큰의 조건부 확률을 Transformer Decoder로 모델링
</Formula>

<Formula title="미세 조정: 보조 언어 모델링 목적 함수">
미세 조정 시 분류 목적 함수에 언어 모델링 목적 함수를 보조 손실로 추가:

$$L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C})$$

여기서:
- $L_2(\mathcal{C}) = \sum_{(x,y)} \log P(y \mid x^1, ..., x^m)$: 분류 목적 함수
- $L_1(\mathcal{C})$: 미세 조정 데이터에 대한 언어 모델링 손실
- $\lambda$: 보조 손실 가중치 (논문에서 0.5 사용)

보조 언어 모델링 손실이 (a) 일반화를 개선하고 (b) 수렴을 가속화한다.
</Formula>

<Formula title="입력 변환 (Input Transformation)">
다양한 태스크를 **동일한 모델 구조**로 처리하기 위해 입력 형식을 변환:

- **분류**: [Start] 텍스트 [Extract] → 선형 분류
- **함의 (Entailment)**: [Start] 전제 [Delim] 가설 [Extract]
- **유사도**: [Start] 문장A [Delim] 문장B [Extract] + 순서 반전 후 합산
- **객관식**: [Start] 컨텍스트 [Delim] 보기$_k$ [Extract] (각 보기별)

모델 구조 변경 없이 **입력 형식만 바꿔** 다양한 태스크 처리.
</Formula>

## 모델 구조

| 하이퍼파라미터 | 값 |
|---|---|
| Transformer 레이어 | 12 |
| 은닉 차원 $d_{\text{model}}$ | 768 |
| Attention Head | 12 |
| FFN 내부 차원 | 3072 |
| 전체 파라미터 | ~117M |
| 컨텍스트 윈도우 | 512 토큰 |
| 사전 학습 데이터 | BooksCorpus (~7,000권, ~800M 단어) |
| 학습 기간 | 약 1개월, 8 GPU |

## 실험 결과

| 벤치마크 | 이전 SOTA | GPT-1 |
|---|---|---|
| SST-2 (감성 분류) | 93.2 | **91.3** |
| MNLI (자연어 추론) | 80.6 | **82.1** |
| QNLI (QA NLI) | 82.3 | **88.1** |
| QQP (문장 유사도) | 66.1 | **70.3** |
| RACE (독해) | 53.3 | **59.0** |
| Story Cloze (상식) | 77.6 | **86.5** |

- **12개 태스크 중 9개**에서 당시 SOTA 달성
- Story Cloze에서 **+8.9%p** 라는 큰 폭의 개선 — 상식 추론 능력 시사
- RACE 독해 태스크에서 **+5.7%p** 개선
- 사전 학습 없이 직접 학습한 Transformer 대비 일관되게 우수 — 사전 학습의 효과 입증

<Impact>
**"사전 학습 → 미세 조정" 패러다임의 첫 성공적 사례이자 GPT 시리즈의 시작.**

- **GPT-2**(2019): 더 큰 모델 + 더 많은 데이터로 zero-shot 능력 발현
- **GPT-3**(2020): 175B 파라미터, few-shot 학습의 가능성 제시
- **ChatGPT**(2022), **GPT-4**(2023): 현재의 LLM 혁명으로 직결
- BERT(2018)와 함께 **"사전 학습 시대"** 를 열었으며, 두 접근이 상호 보완적으로 발전
- **Decoder-only 아키텍처**가 생성 AI의 주류가 됨 — GPT, LLaMA, Claude 등 모든 현대 LLM의 기본 구조
- "Scale이 곧 성능"이라는 스케일링 가설의 초기 증거 제공
- 인용 수 1만회 이상
</Impact>
