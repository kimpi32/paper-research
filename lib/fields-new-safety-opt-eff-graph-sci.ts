import type { Paper } from "./types";

export const newPapers: Record<string, Paper[]> = {
  "safety": [
    {
      id: "adversarial-examples",
      title: "Explaining and Harnessing Adversarial Examples",
      titleKo: "적대적 예제의 설명과 활용",
      authors: ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"],
      year: 2014,
      venue: "ICLR 2015",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/1412.6572",
      tags: [],
      status: "complete",
      citations: "15,000+",
    },
    {
      id: "certified-defenses",
      title: "Certified Adversarial Robustness via Randomized Smoothing",
      titleKo: "랜덤 평활화를 통한 인증된 적대적 강건성",
      authors: ["Jeremy Cohen", "Elan Rosenfeld", "J. Zico Kolter"],
      year: 2019,
      venue: "ICML 2019",
      venueType: "icml",
      arxivUrl: "https://arxiv.org/abs/1902.02918",
      tags: [],
      status: "complete",
      citations: "2,000+",
    },
    {
      id: "truthfulqa",
      title: "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      titleKo: "TruthfulQA: 모델이 인간의 거짓을 모방하는 정도 측정",
      authors: ["Stephanie Lin", "Jacob Hilton", "Owain Evans"],
      year: 2021,
      venue: "ACL 2022",
      venueType: "acl",
      arxivUrl: "https://arxiv.org/abs/2109.07958",
      tags: [],
      status: "complete",
      citations: "1,500+",
    },
    {
      id: "red-teaming",
      title: "Red Teaming Language Models to Reduce Harms",
      titleKo: "피해 감소를 위한 언어 모델 레드팀 테스팅",
      authors: ["Deep Ganguli", "Liane Lovitt", "Jackson Kernion", "et al."],
      year: 2022,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2209.07858",
      tags: [],
      status: "complete",
      citations: "800+",
    },
    {
      id: "representation-engineering",
      title: "Representation Engineering: A Top-Down Approach to AI Transparency",
      titleKo: "표현 공학: AI 투명성에 대한 탑다운 접근법",
      authors: ["Andy Zou", "Long Phan", "Sarah Chen", "et al."],
      year: 2023,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2310.01405",
      tags: [],
      status: "complete",
      citations: "400+",
    },
    {
      id: "weak-to-strong",
      title: "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
      titleKo: "약-강 일반화: 약한 감독으로 강한 능력 이끌어내기",
      authors: ["Collin Burns", "Haotian Ye", "Dan Klein", "Jacob Steinhardt"],
      year: 2023,
      venue: "arXiv (OpenAI)",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2312.09390",
      tags: [],
      status: "complete",
      citations: "300+",
    },
    {
      id: "sleeper-agents",
      title: "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
      titleKo: "슬리퍼 에이전트: 안전 훈련에도 지속되는 기만적 LLM 훈련",
      authors: ["Evan Hubinger", "Carson Denison", "Jesse Mu", "et al."],
      year: 2024,
      venue: "arXiv (Anthropic)",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2401.05566",
      tags: [],
      status: "complete",
      citations: "300+",
    },
  ],
  "optimization": [
    {
      id: "gradient-checkpointing",
      title: "Training Deep Nets with Sublinear Memory Cost",
      titleKo: "서브선형 메모리 비용으로 심층 네트워크 학습",
      authors: ["Tianqi Chen", "Bing Xu", "Chiyuan Zhang", "Carlos Guestrin"],
      year: 2016,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/1604.06174",
      tags: [],
      status: "complete",
      citations: "2,000+",
    },
    {
      id: "lars",
      title: "Large Batch Training of Convolutional Networks",
      titleKo: "합성곱 네트워크의 대규모 배치 학습",
      authors: ["Yang You", "Igor Gitman", "Boris Ginsburg"],
      year: 2017,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/1708.03888",
      tags: [],
      status: "complete",
      citations: "2,000+",
    },
    {
      id: "lamb",
      title: "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
      titleKo: "딥러닝을 위한 대규모 배치 최적화: BERT 76분 학습",
      authors: ["Yang You", "Jing Li", "Sashank Reddi", "et al."],
      year: 2019,
      venue: "ICLR 2020",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/1904.00962",
      tags: [],
      status: "complete",
      citations: "1,500+",
    },
    {
      id: "sam-optimizer",
      title: "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      titleKo: "일반화를 효율적으로 개선하기 위한 샤프니스 인식 최소화",
      authors: ["Pierre Foret", "Ariel Kleiner", "Hossein Mobahi", "Behnam Neyshabur"],
      year: 2020,
      venue: "ICLR 2021",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/2010.01412",
      tags: [],
      status: "complete",
      citations: "3,000+",
    },
    {
      id: "sophia",
      title: "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training",
      titleKo: "Sophia: 언어 모델 사전학습을 위한 확장 가능한 확률적 2차 최적화기",
      authors: ["Hong Liu", "Zhiyuan Li", "David Hall", "et al."],
      year: 2023,
      venue: "ICLR 2024",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/2305.14342",
      tags: [],
      status: "complete",
      citations: "300+",
    },
    {
      id: "mup",
      title: "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
      titleKo: "텐서 프로그램 V: 제로샷 하이퍼파라미터 전이를 통한 대규모 신경망 튜닝",
      authors: ["Greg Yang", "Edward J. Hu", "Igor Babuschkin", "et al."],
      year: 2022,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2203.03466",
      tags: [],
      status: "complete",
      citations: "500+",
    },
    {
      id: "schedule-free",
      title: "The Road Less Scheduled",
      titleKo: "스케줄 없는 길",
      authors: ["Aaron Defazio", "Xingyu Alice Yang", "Harsh Mehta", "et al."],
      year: 2024,
      venue: "arXiv (Meta)",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2405.15682",
      tags: [],
      status: "complete",
      citations: "100+",
    },
  ],
  "efficient": [
    {
      id: "moe",
      title: "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      titleKo: "Switch Transformers: 단순하고 효율적인 희소성으로 조 단위 파라미터 모델로 스케일링",
      authors: ["William Fedus", "Barret Zoph", "Noam Shazeer"],
      year: 2022,
      venue: "JMLR 2022",
      venueType: "jmlr",
      arxivUrl: "https://arxiv.org/abs/2101.03961",
      tags: [],
      status: "complete",
      citations: "3,000+",
    },
    {
      id: "flash-attention",
      title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      titleKo: "FlashAttention: IO 인식 기반의 빠르고 메모리 효율적인 정확한 어텐션",
      authors: ["Tri Dao", "Daniel Y. Fu", "Stefano Ermon", "et al."],
      year: 2022,
      venue: "NeurIPS 2022",
      venueType: "neurips",
      arxivUrl: "https://arxiv.org/abs/2205.14135",
      tags: [],
      status: "complete",
      citations: "3,000+",
    },
    {
      id: "gptq",
      title: "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
      titleKo: "GPTQ: 생성적 사전학습 트랜스포머의 정확한 사후 학습 양자화",
      authors: ["Elias Frantar", "Saleh Ashkboos", "Torsten Hoefler", "Dan Alistarh"],
      year: 2022,
      venue: "ICLR 2023",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/2210.17323",
      tags: [],
      status: "complete",
      citations: "1,500+",
    },
    {
      id: "awq",
      title: "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
      titleKo: "AWQ: LLM 압축과 가속을 위한 활성화 인식 가중치 양자화",
      authors: ["Ji Lin", "Jiaming Tang", "Haotian Tang", "et al."],
      year: 2023,
      venue: "MLSys 2024",
      venueType: "other",
      arxivUrl: "https://arxiv.org/abs/2306.00978",
      tags: [],
      status: "complete",
      citations: "800+",
    },
    {
      id: "speculative-decoding",
      title: "Fast Inference from Transformers via Speculative Decoding",
      titleKo: "추측적 디코딩을 통한 트랜스포머의 빠른 추론",
      authors: ["Yaniv Leviathan", "Matan Kalman", "Yossi Matias"],
      year: 2023,
      venue: "ICML 2023",
      venueType: "icml",
      arxivUrl: "https://arxiv.org/abs/2211.17192",
      tags: [],
      status: "complete",
      citations: "600+",
    },
    {
      id: "vllm",
      title: "Efficient Memory Management for Large Language Model Serving with PagedAttention",
      titleKo: "PagedAttention을 이용한 대규모 언어 모델 서빙의 효율적 메모리 관리",
      authors: ["Woosuk Kwon", "Zhuohan Li", "Siyuan Zhuang", "et al."],
      year: 2023,
      venue: "SOSP 2023",
      venueType: "other",
      arxivUrl: "https://arxiv.org/abs/2309.06180",
      tags: [],
      status: "complete",
      citations: "1,000+",
    },
    {
      id: "flash-attention-2",
      title: "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
      titleKo: "FlashAttention-2: 개선된 병렬화와 작업 분할로 더 빠른 어텐션",
      authors: ["Tri Dao"],
      year: 2023,
      venue: "ICLR 2024",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/2307.08691",
      tags: [],
      status: "complete",
      citations: "800+",
    },
  ],
  "graph": [
    {
      id: "mpnn",
      title: "Neural Message Passing for Quantum Chemistry",
      titleKo: "양자 화학을 위한 신경 메시지 전달",
      authors: ["Justin Gilmer", "Samuel S. Schoenholz", "Patrick F. Riley", "et al."],
      year: 2017,
      venue: "ICML 2017",
      venueType: "icml",
      arxivUrl: "https://arxiv.org/abs/1704.01212",
      tags: [],
      status: "complete",
      citations: "5,000+",
    },
    {
      id: "schnet",
      title: "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions",
      titleKo: "SchNet: 양자 상호작용 모델링을 위한 연속 필터 합성곱 신경망",
      authors: ["Kristof T. Schütt", "Pieter-Jan Kindermans", "Huziel E. Sauceda", "et al."],
      year: 2018,
      venue: "NeurIPS 2017",
      venueType: "neurips",
      arxivUrl: "https://arxiv.org/abs/1706.08566",
      tags: [],
      status: "complete",
      citations: "3,000+",
    },
    {
      id: "gin",
      title: "How Powerful are Graph Neural Networks?",
      titleKo: "그래프 신경망은 얼마나 강력한가?",
      authors: ["Keyulu Xu", "Weihua Hu", "Jure Leskovec", "Stefanie Jegelka"],
      year: 2019,
      venue: "ICLR 2019",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/1810.00826",
      tags: [],
      status: "complete",
      citations: "5,000+",
    },
    {
      id: "ogb",
      title: "Open Graph Benchmark: Datasets for Machine Learning on Graphs",
      titleKo: "오픈 그래프 벤치마크: 그래프 머신러닝을 위한 데이터셋",
      authors: ["Weihua Hu", "Matthias Fey", "Marinka Zitnik", "et al."],
      year: 2020,
      venue: "NeurIPS 2020",
      venueType: "neurips",
      tags: ["benchmark"],
      arxivUrl: "https://arxiv.org/abs/2005.00687",
      status: "complete",
      citations: "2,500+",
    },
    {
      id: "egnn",
      title: "E(n) Equivariant Graph Neural Networks",
      titleKo: "E(n) 등변 그래프 신경망",
      authors: ["Victor Garcia Satorras", "Emiel Hoogeboom", "Max Welling"],
      year: 2021,
      venue: "ICML 2021",
      venueType: "icml",
      arxivUrl: "https://arxiv.org/abs/2102.09844",
      tags: [],
      status: "complete",
      citations: "1,500+",
    },
    {
      id: "graphgps",
      title: "Recipe for a General, Powerful, Scalable Graph Transformer",
      titleKo: "범용적이고 강력하며 확장 가능한 그래프 트랜스포머의 레시피",
      authors: ["Ladislav Rampášek", "Mikhail Galkin", "Vijay Prakash Dwivedi", "et al."],
      year: 2022,
      venue: "NeurIPS 2022",
      venueType: "neurips",
      arxivUrl: "https://arxiv.org/abs/2205.12454",
      tags: [],
      status: "complete",
      citations: "500+",
    },
    {
      id: "graphmae",
      title: "GraphMAE: Self-Supervised Masked Graph Autoencoders",
      titleKo: "GraphMAE: 자기지도 마스크 그래프 오토인코더",
      authors: ["Zhenyu Hou", "Xiao Liu", "Yukuo Cen", "et al."],
      year: 2022,
      venue: "KDD 2022",
      venueType: "other",
      arxivUrl: "https://arxiv.org/abs/2205.10803",
      tags: [],
      status: "complete",
      citations: "600+",
    },
  ],
  "science": [
    {
      id: "neural-ode",
      title: "Neural Ordinary Differential Equations",
      titleKo: "신경 상미분방정식",
      authors: ["Ricky T. Q. Chen", "Yulia Rubanova", "Jesse Bettencourt", "David Duvenaud"],
      year: 2018,
      venue: "NeurIPS 2018",
      venueType: "neurips",
      award: "best-paper",
      arxivUrl: "https://arxiv.org/abs/1806.07366",
      tags: [],
      status: "complete",
      citations: "8,000+",
    },
    {
      id: "pinns",
      title: "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems",
      titleKo: "물리 정보 신경망: 정방향 및 역방향 문제를 풀기 위한 딥러닝 프레임워크",
      authors: ["Maziar Raissi", "Paris Perdikaris", "George Em Karniadakis"],
      year: 2019,
      venue: "Journal of Computational Physics",
      venueType: "other",
      conferenceUrl: "https://doi.org/10.1016/j.jcp.2018.10.045",
      tags: [],
      status: "complete",
      citations: "10,000+",
    },
    {
      id: "alphafold1",
      title: "Improved protein structure prediction using potentials from deep learning",
      titleKo: "딥러닝 포텐셜을 이용한 개선된 단백질 구조 예측",
      authors: ["Andrew W. Senior", "Richard Evans", "John Jumper", "et al."],
      year: 2020,
      venue: "Nature",
      venueType: "nature",
      conferenceUrl: "https://doi.org/10.1038/s41586-019-1923-7",
      tags: [],
      status: "complete",
      citations: "5,000+",
    },
    {
      id: "esmfold",
      title: "Evolutionary-scale prediction of atomic-level protein structure with a language model",
      titleKo: "언어 모델을 이용한 진화적 규모의 원자 수준 단백질 구조 예측",
      authors: ["Zeming Lin", "Halil Akin", "Roshan Rao", "et al."],
      year: 2022,
      venue: "Science",
      venueType: "science",
      conferenceUrl: "https://doi.org/10.1126/science.ade2574",
      tags: [],
      status: "complete",
      citations: "2,000+",
    },
    {
      id: "fourcastnet",
      title: "FourCastNet: A Global Data-driven High-resolution Weather Forecasting Model",
      titleKo: "FourCastNet: 글로벌 데이터 기반 고해상도 기상 예보 모델",
      authors: ["Jaideep Pathak", "Shashank Subramanian", "Peter Harrington", "et al."],
      year: 2022,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2202.11214",
      tags: [],
      status: "complete",
      citations: "800+",
    },
    {
      id: "diffdock",
      title: "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",
      titleKo: "DiffDock: 분자 도킹을 위한 확산 모델",
      authors: ["Gabriele Corso", "Hannes Stärk", "Bowen Jing", "et al."],
      year: 2023,
      venue: "ICLR 2023",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/2210.01776",
      tags: [],
      status: "complete",
      citations: "700+",
    },
    {
      id: "gnome",
      title: "Scaling deep learning for materials discovery",
      titleKo: "재료 발견을 위한 딥러닝 스케일링",
      authors: ["Amil Merchant", "Simon Batzner", "Samuel S. Schoenholz", "et al."],
      year: 2023,
      venue: "Nature",
      venueType: "nature",
      conferenceUrl: "https://doi.org/10.1038/s41586-023-06735-9",
      tags: [],
      status: "complete",
      citations: "800+",
    },
    {
      id: "evo",
      title: "Sequence modeling and design from molecular to genome scale with Evo",
      titleKo: "Evo: 분자에서 게놈 규모까지의 시퀀스 모델링과 설계",
      authors: ["Eric Nguyen", "Michael Poli", "Matthew G. Durrant", "et al."],
      year: 2024,
      venue: "Science",
      venueType: "science",
      conferenceUrl: "https://doi.org/10.1126/science.ado9336",
      tags: [],
      status: "complete",
      citations: "200+",
    },
  ],
};
