import type { Paper } from "./types";

// New papers to add, grouped by fieldId
export const newPapers: Record<string, Paper[]> = {
  "nlp": [
    {
      id: "glove",
      title: "GloVe: Global Vectors for Word Representation",
      titleKo: "GloVe: 단어 표현을 위한 글로벌 벡터",
      authors: ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"],
      year: 2014,
      venue: "EMNLP 2014",
      venueType: "emnlp",
      arxivUrl: "https://nlp.stanford.edu/pubs/glove.pdf",
      tags: [],
      status: "complete",
      citations: "30,000+",
    },
    {
      id: "bart",
      title: "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      titleKo: "BART: 자연어 생성, 번역, 이해를 위한 디노이징 시퀀스-투-시퀀스 사전학습",
      authors: ["Mike Lewis", "Yinhan Liu", "Naman Goyal", "et al."],
      year: 2020,
      venue: "ACL 2020",
      venueType: "acl",
      arxivUrl: "https://arxiv.org/abs/1910.13461",
      tags: [],
      status: "complete",
      citations: "10,000+",
    },
    {
      id: "roberta",
      title: "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      titleKo: "RoBERTa: 강건하게 최적화된 BERT 사전학습 접근법",
      authors: ["Yinhan Liu", "Myle Ott", "Naman Goyal", "et al."],
      year: 2019,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/1907.11692",
      tags: [],
      status: "complete",
      citations: "18,000+",
    },
    {
      id: "deberta",
      title: "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
      titleKo: "DeBERTa: 분리된 어텐션을 활용한 디코딩 강화 BERT",
      authors: ["Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen"],
      year: 2020,
      venue: "ICLR 2021",
      venueType: "iclr",
      arxivUrl: "https://arxiv.org/abs/2006.03654",
      tags: [],
      status: "complete",
      citations: "4,000+",
    },
    {
      id: "flan-t5",
      title: "Scaling Instruction-Finetuned Language Models",
      titleKo: "지시 미세조정 언어 모델의 스케일링",
      authors: ["Hyung Won Chung", "Le Hou", "Shayne Longpre", "et al."],
      year: 2022,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2210.11416",
      tags: [],
      status: "complete",
      citations: "5,000+",
    },
  ],
  "llm": [
    {
      id: "codex",
      title: "Evaluating Large Language Models Trained on Code",
      titleKo: "코드로 학습된 대규모 언어 모델 평가",
      authors: ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "et al."],
      year: 2021,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2107.03374",
      tags: [],
      status: "complete",
      citations: "5,000+",
    },
    {
      id: "palm",
      title: "PaLM: Scaling Language Modeling with Pathways",
      titleKo: "PaLM: Pathways를 활용한 언어 모델링 스케일링",
      authors: ["Aakanksha Chowdhery", "Sharan Narang", "Jacob Devlin", "et al."],
      year: 2022,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2204.02311",
      tags: [],
      status: "complete",
      citations: "7,000+",
    },
    {
      id: "llama2",
      title: "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      titleKo: "Llama 2: 개방형 파운데이션 및 미세조정 채팅 모델",
      authors: ["Hugo Touvron", "Louis Martin", "Kevin Stone", "et al."],
      year: 2023,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2307.09288",
      tags: [],
      status: "complete",
      citations: "10,000+",
    },
    {
      id: "mixtral",
      title: "Mixtral of Experts",
      titleKo: "Mixtral: 전문가 혼합 모델",
      authors: ["Albert Q. Jiang", "Alexandre Sablayrolles", "Antoine Roux", "et al."],
      year: 2024,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2401.04088",
      tags: [],
      status: "complete",
      citations: "2,000+",
    },
    {
      id: "gemma",
      title: "Gemma: Open Models Based on Gemini Research and Technology",
      titleKo: "Gemma: Gemini 연구와 기술 기반의 오픈 모델",
      authors: ["Gemma Team", "Thomas Mesnard", "Cassidy Hardin", "et al."],
      year: 2024,
      venue: "arXiv",
      venueType: "arxiv",
      arxivUrl: "https://arxiv.org/abs/2403.08295",
      tags: [],
      status: "complete",
      citations: "1,000+",
    },
  ],
};
