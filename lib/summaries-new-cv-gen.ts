import type { PaperSummary } from "./paper-summaries";

export const newCvGenSummaries: Record<string, PaperSummary> = {
  // ============================================================
  // CV Field (cv) - Additional Papers
  // ============================================================

  "inception": {
    tldr: "1x1, 3x3, 5x5 합성곱을 병렬로 수행하는 인셉션 모듈을 도입하여 AlexNet 대비 파라미터를 12배 줄이면서도 22층 깊이의 네트워크로 ILSVRC 2014에서 우승한 모델이다.",
    background: "2014년 시점에서 딥러닝 모델의 성능을 높이려면 네트워크를 더 깊고 넓게 만들어야 했지만, 이는 파라미터 수의 폭증과 과적합, 계산 비용 증가라는 문제를 수반했다. AlexNet(6천만 파라미터)이후 네트워크를 단순히 크게 만드는 접근법은 한계에 도달하고 있었으며, 제한된 계산 자원 내에서 네트워크의 표현력을 극대화하는 효율적인 아키텍처 설계가 필요했다.",
    keyIdea: "GoogLeNet/Inception은 하나의 레이어에서 여러 크기의 합성곱 필터(1x1, 3x3, 5x5)와 맥스 풀링을 병렬로 적용하고 그 결과를 채널 방향으로 결합하는 '인셉션 모듈'을 제안한다. 이를 통해 네트워크가 각 레이어에서 다양한 스케일의 특징을 동시에 포착할 수 있다. 핵심적으로 3x3, 5x5 합성곱 전에 1x1 합성곱을 배치하여 채널 수를 줄이는 차원 축소(bottleneck)를 적용함으로써, 계산량을 극적으로 감소시킨다. 이 설계 덕분에 22층이라는 당시로서는 매우 깊은 네트워크를 AlexNet의 1/12 파라미터(약 500만 개)로 구현할 수 있었다.",
    method: "9개의 인셉션 모듈을 쌓아 총 22층의 네트워크를 구성한다. 각 인셉션 모듈 내에서 1x1 합성곱이 차원 축소 역할을 수행한 뒤 3x3, 5x5 합성곱이 적용된다. 학습 시 중간 레이어에 보조 분류기(auxiliary classifier)를 두어 깊은 네트워크에서의 기울기 소실 문제를 완화한다. 글로벌 평균 풀링을 최종 분류기 이전에 사용하여 완전연결 레이어의 파라미터를 크게 줄인다.",
    results: "ILSVRC 2014 분류 챌린지에서 top-5 오류율 6.67%를 달성하며 1위를 차지했다. AlexNet(약 6천만 파라미터) 대비 12배 적은 파라미터로 훨씬 우수한 성능을 보여, 효율적 아키텍처 설계의 중요성을 입증했다.",
    impact: "다중 스케일 특징 추출이라는 설계 원칙과 1x1 합성곱을 활용한 차원 축소 기법은 이후 거의 모든 CNN 아키텍처에 영향을 미쳤다. Inception v2/v3/v4로 지속 발전했으며, 효율적인 네트워크 설계라는 연구 방향(MobileNet, EfficientNet 등)의 선구적 역할을 했다. CVPR 2015 Best Paper로 선정되었다.",
    relatedFoundations: ["alexnet", "batch-normalization"],
    relatedPapers: [
      { id: "efficientnet", fieldId: "cv", title: "EfficientNet", relation: "successor" },
    ],
  },

  "fpn": {
    tldr: "고수준 특징 맵의 의미 정보를 저수준으로 전파하는 하향식 경로와 측면 연결을 통해 모든 스케일에서 풍부한 특징을 제공하는 다중 스케일 특징 추출 아키텍처이다.",
    background: "객체 검출에서 다양한 크기의 객체를 인식하는 것은 핵심 과제였다. 기존 방법들은 이미지 피라미드(입력 이미지를 여러 해상도로 리사이즈)를 사용했지만 계산 비용이 매우 높았다. CNN의 자연스러운 특징 계층 구조를 활용하려는 시도가 있었지만, 저수준 특징 맵은 의미 정보가 부족하고 고수준 특징 맵은 공간 해상도가 낮아 작은 객체 검출에 불리했다.",
    keyIdea: "FPN은 CNN 백본의 계층적 특징 맵을 활용하여 상향식(bottom-up) 경로와 하향식(top-down) 경로를 결합한 피라미드를 구축한다. 상향식 경로는 일반적인 CNN 순전파로 각 스케일의 특징을 추출하고, 하향식 경로는 가장 고수준(가장 작은) 특징 맵부터 시작해 2배 업샘플링하며 해당 스케일의 상향식 특징과 1x1 합성곱을 통한 측면 연결(lateral connection)로 결합한다. 이렇게 하면 모든 스케일의 특징 맵이 강한 의미 정보와 높은 공간 해상도를 동시에 갖게 되어, 크고 작은 객체를 모두 효과적으로 검출할 수 있다.",
    method: "ResNet 등의 백본에서 conv2~conv5 단계의 특징 맵을 추출한다. 하향식 경로에서는 상위 특징 맵을 2배 최근접 이웃 업샘플링한 뒤, 대응하는 하위 특징 맵을 1x1 합성곱으로 채널 수를 맞추어 요소별 합산한다. 각 합산된 특징 맵에 3x3 합성곱을 적용하여 앨리어싱을 줄인 최종 피라미드 레벨을 생성한다. RPN과 검출 헤드가 모든 피라미드 레벨에 공유 가중치로 적용된다.",
    results: "COCO 객체 검출에서 단일 모델로 기존 최고 성능을 넘어섰으며, 특히 작은 객체 검출에서 큰 성능 향상을 보였다. 이미지 피라미드 없이도 다중 스케일 검출이 가능하여 추론 효율성도 확보했다. Faster R-CNN에 FPN을 적용했을 때 AP가 2.0 이상 개선되었다.",
    impact: "FPN의 하향식 경로 + 측면 연결 패턴은 이후 객체 검출과 분할 분야의 사실상 표준이 되었다. Mask R-CNN, RetinaNet, Panoptic FPN 등 주요 검출/분할 프레임워크가 FPN 위에 구축되었으며, PANet, NAS-FPN, BiFPN 등 다양한 개선 변형이 제안되었다. 다중 스케일 특징 융합이라는 설계 원칙은 분할, 키포인트 검출, 깊이 추정 등 광범위한 비전 태스크에 적용되고 있다.",
    relatedFoundations: ["resnet"],
    relatedPapers: [
      { id: "mask-rcnn", fieldId: "cv", title: "Mask R-CNN", relation: "related" },
      { id: "detr", fieldId: "cv", title: "DETR", relation: "successor" },
    ],
  },

  "efficientnet": {
    tldr: "네트워크의 깊이, 너비, 입력 해상도를 복합적으로 균형 있게 스케일링하는 원칙을 제시하고, NAS로 찾은 기본 모델을 체계적으로 확장하여 기존 CNN 대비 훨씬 적은 파라미터로 최고 성능을 달성한 모델이다.",
    background: "CNN 성능을 높이기 위해 네트워크를 깊게(ResNet), 넓게(WideResNet), 또는 높은 해상도의 입력을 사용하는 방법이 각각 연구되었지만, 이 세 차원을 어떻게 균형 있게 조합해야 최적인지에 대한 체계적 이해가 부족했다. 대부분의 모델은 이 중 하나의 차원만 임의로 확장하여 효율성이 떨어졌으며, 제한된 계산 자원 내에서 최적의 모델 구조를 찾는 것이 중요한 과제였다.",
    keyIdea: "EfficientNet은 네트워크 스케일링에 대한 핵심적 관찰을 제시한다: 깊이(depth), 너비(width), 해상도(resolution)의 세 차원은 서로 밀접하게 연관되어 있으며, 하나만 확장하면 빠르게 수익이 체감한다. 복합 스케일링(compound scaling) 방법은 하나의 복합 계수 φ를 사용하여 세 차원을 고정된 비율(α, β, γ)로 동시에 균형 있게 확장한다. 기본 네트워크(EfficientNet-B0)는 Neural Architecture Search(NAS)로 탐색하여 찾고, 이를 복합 스케일링으로 B1~B7까지 체계적으로 확장한다.",
    method: "먼저 MnasNet과 유사한 다목적 NAS를 수행하여 정확도와 FLOPS를 동시에 최적화하는 기본 아키텍처 B0를 탐색한다. 그리드 탐색으로 깊이(α), 너비(β), 해상도(γ)의 최적 비율을 결정한 뒤, 복합 계수 φ를 증가시키며 α^φ, β^φ, γ^φ로 세 차원을 동시에 확장한다. 주요 구성 요소는 MBConv(모바일 역잔차 블록)와 squeeze-and-excitation 모듈이다.",
    results: "EfficientNet-B7은 ImageNet top-1 정확도 84.3%를 달성하며 당시 최고 성능을 기록했다. 기존 최고 모델(GPipe) 대비 8.4배 작고 6.1배 빠르면서도 더 높은 정확도를 보였다. B0~B7까지 일관되게 파라미터 대비 최고의 효율성을 입증했다.",
    impact: "모델 스케일링에 대한 체계적 프레임워크를 제시하여 이후 효율적 아키텍처 연구의 핵심 참조가 되었다. EfficientNetV2에서 학습 속도까지 고려한 개선이 이루어졌으며, NAS 기반 설계와 체계적 스케일링의 결합이라는 패러다임은 모바일/엣지 컴퓨팅 분야에도 큰 영향을 미쳤다. 산업계에서도 효율적인 비전 백본으로 널리 채택되었다.",
    relatedFoundations: ["resnet", "batch-normalization"],
    relatedPapers: [
      { id: "inception", fieldId: "cv", title: "Inception", relation: "prior" },
      { id: "convnext", fieldId: "cv", title: "ConvNeXt", relation: "successor" },
    ],
  },

  "swin-transformer": {
    tldr: "시프티드 윈도우(shifted window) 기반 자기 어텐션으로 계층적 특징 맵을 구축하여 비전 트랜스포머의 계산 복잡도를 선형으로 줄이고 다양한 비전 태스크에서 범용 백본으로 사용 가능하게 한 모델이다.",
    background: "ViT가 비전에서 트랜스포머의 가능성을 보여주었지만, 이미지 전체에 대한 전역 자기 어텐션은 이미지 크기의 제곱에 비례하는 계산 복잡도를 가져 고해상도 입력이나 밀집 예측(dense prediction) 태스크에 적용하기 어려웠다. 또한 ViT는 단일 해상도의 특징 맵만 생성하여 객체 검출이나 분할처럼 다중 스케일 특징이 필요한 태스크에 직접 사용하기 불편했다.",
    keyIdea: "Swin Transformer는 두 가지 핵심 설계를 통해 이러한 문제를 해결한다. 첫째, 이미지를 겹치지 않는 로컬 윈도우(예: 7x7 패치)로 분할하고 각 윈도우 내에서만 자기 어텐션을 수행하여 계산 복잡도를 이미지 크기에 선형으로 만든다. 둘째, 연속된 트랜스포머 블록에서 윈도우 분할을 절반 크기만큼 이동(shift)시켜, 인접 윈도우 간 정보 교환을 가능하게 한다. 또한 패치 병합(patch merging) 레이어로 특징 맵 해상도를 점진적으로 줄이면서 채널 수를 늘려, CNN과 유사한 계층적 특징 피라미드를 생성한다.",
    method: "입력 이미지를 4x4 패치로 분할한 후, 각 스테이지에서 Swin Transformer 블록들을 적용한다. 각 블록은 윈도우 기반 멀티헤드 자기 어텐션(W-MSA)과 시프티드 윈도우 멀티헤드 자기 어텐션(SW-MSA)을 교대로 사용한다. 패치 병합으로 2x2 인접 패치를 합쳐 해상도를 절반으로 줄이며, 4개 스테이지를 거쳐 1/4, 1/8, 1/16, 1/32 해상도의 계층적 특징 맵을 생성한다. 상대적 위치 바이어스를 어텐션에 추가하여 위치 정보를 인코딩한다.",
    results: "ImageNet-1K에서 87.3% top-1 정확도를 달성했으며, COCO 객체 검출에서 58.7 box AP, ADE20K 시맨틱 분할에서 53.5 mIoU를 기록하여 이전 최고 성능을 큰 폭으로 갱신했다. 모든 태스크에서 CNN과 이전 비전 트랜스포머를 능가하는 범용적 우수성을 입증했다.",
    impact: "비전 트랜스포머가 분류를 넘어 검출, 분할 등 모든 비전 태스크의 범용 백본으로 사용될 수 있음을 입증했다. ICCV 2021 Best Paper로 선정되었으며, 윈도우 기반 어텐션은 이후 비전 트랜스포머 연구의 핵심 설계 요소가 되었다. Swin V2, Video Swin Transformer 등으로 확장되었고, ConvNeXt 등 경쟁 아키텍처의 중요한 비교 기준이 되었다.",
    relatedFoundations: ["transformer", "vit"],
    relatedPapers: [
      { id: "convnext", fieldId: "cv", title: "ConvNeXt", relation: "related" },
      { id: "detr", fieldId: "cv", title: "DETR", relation: "related" },
    ],
  },

  "convnext": {
    tldr: "Swin Transformer의 설계 원칙들을 순수 합성곱 네트워크에 체계적으로 적용하여, 트랜스포머와 동등하거나 우월한 성능을 달성한 '현대화된' ConvNet이다.",
    background: "ViT와 Swin Transformer의 성공 이후, 비전 분야에서 트랜스포머가 CNN을 대체하는 것이 대세로 여겨졌다. 그러나 트랜스포머의 우수한 성능이 자기 어텐션 메커니즘 자체 때문인지, 아니면 학습 전략이나 매크로/마이크로 설계 선택 때문인지는 명확하지 않았다. 순수 CNN으로도 유사한 성능을 달성할 수 있는지 체계적으로 검증할 필요가 있었다.",
    keyIdea: "ConvNeXt는 표준 ResNet에서 출발하여 Swin Transformer의 설계 요소들을 하나씩 CNN에 이식하며 점진적으로 성능을 개선한다. 주요 현대화 요소로는 (1) 스테이지별 연산 비율 조정(3:3:9:3 → 3:3:27:3처럼 Swin과 유사하게), (2) 패치화된 스템(patchify stem)으로 큰 스트라이드 사용, (3) ResNeXt 스타일의 분리 합성곱(depthwise convolution), (4) 커널 크기를 7x7로 확대하여 Swin의 7x7 윈도우에 대응, (5) 배치 정규화를 레이어 정규화로 교체, (6) GELU 활성 함수 사용, (7) 활성 함수와 정규화 레이어 수 감소 등이 있다. 이러한 변경들을 통해 순수 합성곱만으로 트랜스포머에 필적하는 성능을 달성한다.",
    method: "ResNet-50에서 출발하여 위의 현대화 요소를 순차적으로 적용하며 각 변경의 영향을 실험적으로 검증한다. 최종 ConvNeXt 블록은 depthwise 7x7 합성곱 → LayerNorm → 1x1 합성곱 → GELU → 1x1 합성곱의 역병목(inverted bottleneck) 구조를 사용한다. 모델 크기별로 T/S/B/L/XL 변형을 제공하며, ImageNet-22K 사전학습도 수행한다.",
    results: "ConvNeXt-T는 ImageNet-1K에서 82.1% top-1 정확도를 달성하여 Swin-T(81.3%)를 능가했다. 더 큰 모델에서도 Swin Transformer와 동등하거나 우수한 성능을 보였으며, COCO 검출과 ADE20K 분할에서도 경쟁력 있는 결과를 보여 순수 CNN의 잠재력을 재확인했다.",
    impact: "CNN이 적절한 현대적 설계 원칙으로 여전히 트랜스포머와 경쟁할 수 있음을 실증적으로 보여주어, '트랜스포머 vs CNN' 논쟁에 중요한 균형점을 제시했다. 단순하고 효율적인 아키텍처로서 산업 현장에서의 실용적 가치도 높으며, ConvNeXt V2로 발전하며 자기 지도학습과의 결합도 연구되었다. 아키텍처 설계에서 '공정한 비교'의 중요성을 일깨워준 연구이다.",
    relatedFoundations: ["resnet", "batch-normalization"],
    relatedPapers: [
      { id: "swin-transformer", fieldId: "cv", title: "Swin Transformer", relation: "related" },
      { id: "efficientnet", fieldId: "cv", title: "EfficientNet", relation: "prior" },
    ],
  },

  "dinov2": {
    tldr: "대규모 큐레이션된 데이터에서 자기 지도학습으로 훈련한 ViT가 파인튜닝 없이도 분류, 분할, 깊이 추정 등 다양한 비전 태스크에서 범용 시각 특징을 제공할 수 있음을 보인 모델이다.",
    background: "NLP에서는 BERT, GPT 등 자기 지도학습으로 사전학습한 모델이 범용 특징 추출기로 성공적으로 사용되고 있었지만, 비전에서는 자기 지도학습 특징의 품질이 지도학습에 미치지 못하여 별도의 파인튜닝이 필수적이었다. DINO가 자기 지도학습 ViT의 가능성을 보여주었지만, ImageNet 규모의 데이터에 한정되어 있었고, 다양한 다운스트림 태스크에서의 범용성은 제한적이었다.",
    keyIdea: "DINOv2는 세 가지 핵심 요소의 결합으로 범용 시각 특징을 달성한다. 첫째, 1.42억 장의 자동 큐레이션된 대규모 데이터셋(LVD-142M)을 구축하여 학습 데이터의 다양성과 품질을 확보한다. 데이터 큐레이션은 이미지 임베딩 기반 중복 제거, 품질 필터링, 다양성 샘플링을 포함한다. 둘째, DINO의 자기 증류(self-distillation)와 iBOT의 마스크 이미지 모델링을 결합한 학습 방법을 사용한다. 셋째, ViT-g(10억 파라미터) 규모까지 안정적으로 학습하기 위한 기술적 개선(Flash Attention, 효율적 stochastic depth 등)을 적용한다.",
    method: "교사 네트워크와 학생 네트워크 간의 자기 증류 프레임워크에서, 글로벌 크롭과 로컬 크롭 간의 크로스뷰 예측(DINO 목적함수)과 마스크된 패치의 특징 예측(iBOT 목적함수)을 동시에 최적화한다. 교사 네트워크는 학생의 지수 이동 평균으로 업데이트된다. KoLeo 정규화로 특징 공간의 균일성을 유지하고, Sinkhorn-Knopp 센터링으로 모드 붕괴를 방지한다. 큰 모델을 먼저 학습한 뒤 작은 모델로 지식 증류하는 전략도 사용한다.",
    results: "파인튜닝 없이 frozen 특징만으로 ImageNet 분류 86.3%(선형 프로빙), ADE20K 분할, NYUd 깊이 추정 등에서 태스크 특화 지도학습 모델을 능가하거나 동등한 성능을 보였다. 의료, 위성, 식물 등 학습 데이터에 포함되지 않은 도메인에서도 우수한 전이 성능을 보여 진정한 범용 특징의 가능성을 입증했다.",
    impact: "비전에서의 자기 지도학습 파운데이션 모델이 NLP의 사전학습 모델과 유사한 수준의 범용성을 달성할 수 있음을 보여주었다. 파인튜닝 없는 범용 특징 추출기로서 연구와 산업 모두에서 채택이 급속히 확대되고 있으며, 3D 재구성, 로봇 비전, 의료 영상 등 다양한 분야에서 기본 백본으로 활용되고 있다. 데이터 큐레이션의 중요성도 재조명했다.",
    relatedFoundations: ["vit"],
    relatedPapers: [
      { id: "dino", fieldId: "representation", title: "DINO", relation: "prior" },
      { id: "sam", fieldId: "cv", title: "SAM", relation: "related" },
    ],
  },

  // ============================================================
  // Generative Field (generative) - Additional Papers
  // ============================================================

  "pix2pix": {
    tldr: "조건부 GAN 프레임워크로 쌍을 이루는(paired) 이미지 간 변환을 학습하여, 엣지→사진, 낮→밤, 스케치→실사 등 다양한 이미지 변환을 범용적으로 수행하는 모델이다.",
    background: "이미지 대 이미지 변환(image-to-image translation)은 컴퓨터 비전의 오래된 과제로, 각 변환 태스크(컬러화, 초해상도, 시맨틱 분할 등)마다 별도의 전문 알고리즘을 설계해야 했다. CNN 기반 방법들은 L1/L2 손실만 사용하면 흐릿한(blurry) 결과를 생성하는 문제가 있었으며, GAN이 사실적 이미지를 생성할 수 있음이 알려졌지만 조건부 생성에 대한 체계적 프레임워크가 부재했다.",
    keyIdea: "pix2pix는 조건부 GAN을 이미지-이미지 변환의 범용 프레임워크로 제안한다. 생성자는 입력 이미지를 받아 대응하는 출력 이미지를 생성하고, 판별자는 입력-출력 쌍이 진짜인지 가짜인지 판별한다. 핵심 설계로 (1) U-Net 기반 생성자가 스킵 연결을 통해 저수준 정보를 보존하고, (2) PatchGAN 판별자가 이미지의 NxN 패치 단위로 진위를 판별하여 고주파 텍스처의 사실감을 향상시킨다. cGAN 손실에 L1 손실을 결합하여 전체 구조의 정확성과 로컬 사실감을 동시에 확보한다.",
    method: "생성자는 인코더-디코더 구조에 U-Net 스킵 연결을 추가한 아키텍처를 사용한다. PatchGAN 판별자는 70x70 수용장(receptive field)의 패치 단위로 진위를 판별하며, 이는 전체 이미지 판별자보다 적은 파라미터로 더 선명한 결과를 생성한다. 목적 함수는 cGAN 손실과 L1 재구성 손실의 가중 합이다. 학습 시 드롭아웃을 노이즈 소스로 활용한다.",
    results: "도시 풍경 라벨↔사진, 건물 파사드, 지도↔항공사진, 흑백→컬러, 엣지→사진, 낮→밤 등 다양한 변환 태스크에서 사실적인 결과를 생성했다. AMT 실험에서 사람 평가자의 상당수가 생성 이미지를 실제로 오인했으며, 하나의 동일한 아키텍처와 목적 함수로 여러 태스크를 처리할 수 있음을 입증했다.",
    impact: "이미지-이미지 변환의 범용 프레임워크를 확립하여 해당 분야의 표준이 되었다. CycleGAN, SPADE, GauGAN 등 수많은 후속 연구의 기반이 되었으며, PatchGAN 판별자는 이후 대부분의 이미지 생성/변환 모델에서 채택되었다. 예술, 디자인, 건축, 의료 영상 등 실용적 응용에서도 널리 사용되었다.",
    relatedFoundations: ["gan"],
    relatedPapers: [
      { id: "cyclegan", fieldId: "generative", title: "CycleGAN", relation: "related" },
      { id: "wgan", fieldId: "generative", title: "WGAN", relation: "related" },
    ],
  },

  "cyclegan": {
    tldr: "쌍을 이루지 않는(unpaired) 두 이미지 도메인 간 변환을 순환 일관성 손실(cycle-consistency loss)을 통해 학습하여, 짝지어진 학습 데이터 없이도 스타일 변환을 수행하는 모델이다.",
    background: "pix2pix 등 기존 이미지 변환 모델은 입출력이 정확히 대응하는 쌍(paired) 데이터를 요구했지만, 실제로 이러한 데이터를 구하기는 매우 어렵거나 불가능한 경우가 많았다(예: 모네 그림↔실제 사진, 얼룩말↔말). 쌍이 없는 두 도메인의 이미지 집합만으로 의미 있는 매핑을 학습하는 것은 극도로 제약이 부족한(underconstrained) 문제였다.",
    keyIdea: "CycleGAN은 순환 일관성(cycle consistency)이라는 우아한 제약 조건을 도입하여 쌍 없는 변환 문제를 해결한다. 도메인 X에서 Y로 변환하는 생성자 G와 Y에서 X로 변환하는 생성자 F를 동시에 학습하며, 핵심 제약은 X→Y→X (또는 Y→X→Y)로 왕복 변환했을 때 원본이 복원되어야 한다는 것이다 (F(G(x)) ≈ x, G(F(y)) ≈ y). 이 순환 일관성 손실은 쌍 데이터 없이도 두 변환이 서로의 역함수에 가까워지도록 강제하여, 의미 있는 도메인 간 매핑을 학습하게 한다. 각 도메인에 대한 적대적 손실과 결합하여 사실적인 변환 결과를 생성한다.",
    method: "두 개의 생성자(G: X→Y, F: Y→X)와 두 개의 판별자(D_X, D_Y)를 학습한다. 전체 목적 함수는 양방향 적대적 손실과 순환 일관성 손실(L1 거리)의 합이다. 생성자는 ResNet 기반 아키텍처(9개 잔차 블록)를 사용하고, 판별자는 PatchGAN(70x70)을 사용한다. 선택적으로 정체성 손실(identity loss)을 추가하여 색상 보존을 개선한다.",
    results: "말↔얼룩말, 사과↔오렌지, 사진↔모네/반 고흐/세잔/우키요에, 여름↔겨울 등 다양한 비짝(unpaired) 변환 태스크에서 시각적으로 설득력 있는 결과를 생성했다. pix2pix(쌍 데이터 사용)에 비해 약간 성능이 낮지만, 쌍 데이터 없이 이를 달성한 점에서 큰 의의가 있다.",
    impact: "쌍 없는 이미지 변환이라는 새로운 패러다임을 열어 비지도 도메인 적응, 스타일 전이, 데이터 증강 등 광범위한 응용을 가능하게 했다. 순환 일관성 개념은 이후 비디오, 3D, 텍스트 등 다른 모달리티의 비지도 변환에도 적용되었으며, StarGAN, MUNIT, FUNIT 등 다중 도메인 변환 연구로 발전하는 토대가 되었다. 예술, 사진 편집, 시뮬레이션-실제 도메인 갭 해소 등에서 널리 활용되고 있다.",
    relatedFoundations: ["gan"],
    relatedPapers: [
      { id: "pix2pix", fieldId: "generative", title: "pix2pix", relation: "related" },
      { id: "stylegan", fieldId: "generative", title: "StyleGAN", relation: "successor" },
    ],
  },

  "vqvae2": {
    tldr: "계층적 벡터 양자화 잠재 공간을 통해 글로벌 구조와 로컬 디테일을 분리하여 GAN에 필적하는 고해상도 이미지를 생성하는 자기회귀 모델이다.",
    background: "VAE 계열 모델은 학습이 안정적이고 잠재 공간이 잘 구조화되지만, 생성 이미지가 흐릿한 문제로 인해 GAN에 비해 시각적 품질이 크게 떨어졌다. VQ-VAE(벡터 양자화 VAE)가 이산 잠재 표현으로 이 문제를 완화했지만, 단일 스케일의 잠재 코드로는 고해상도에서의 복잡한 구조를 충분히 표현하기 어려웠다.",
    keyIdea: "VQ-VAE-2는 계층적(hierarchical) 벡터 양자화를 도입하여 다중 해상도에서 잠재 코드를 학습한다. 상위 레벨의 잠재 맵은 낮은 해상도에서 이미지의 전체적 구조(포즈, 형태, 레이아웃)를 포착하고, 하위 레벨은 높은 해상도에서 세밀한 텍스처와 디테일을 인코딩한다. 생성 시에는 상위→하위 순서로 자기회귀 모델(PixelCNN)을 사용하여 잠재 코드를 샘플링한 뒤, 디코더로 이미지를 재구성한다. 이 2단계 접근법(잠재 코드 학습 + 잠재 공간에서의 자기회귀 생성)은 이후 LDM(Latent Diffusion Models)의 핵심 아이디어와 직결된다.",
    method: "인코더는 입력 이미지를 두 단계의 잠재 맵(예: 32x32 상위, 64x64 하위)으로 인코딩하며, 각 레벨에서 가장 가까운 코드북 벡터로 양자화한다. 하위 레벨은 상위 레벨의 정보를 조건으로 받는다. 디코더는 양자화된 잠재 맵에서 이미지를 복원한다. 생성 단계에서는 대형 PixelCNN을 상위/하위 잠재 맵에 대해 각각 학습하고, 거부 샘플링(rejection sampling)이나 분류기를 사용하여 생성 품질을 높인다.",
    results: "256x256 해상도의 얼굴(FFHQ), ImageNet 클래스 조건부 생성에서 BigGAN에 필적하는 시각적 품질과 다양성을 보였다. 인간 평가에서 참가자의 상당수가 VQ-VAE-2 생성 이미지를 실제 사진으로 판단했으며, likelihood 기반 모델이 GAN과 경쟁할 수 있음을 최초로 입증했다.",
    impact: "우도 기반(likelihood-based) 생성 모델이 시각적 품질에서 GAN과 경쟁할 수 있다는 패러다임 전환을 이끌었다. 잠재 공간에서의 생성이라는 2단계 접근법은 LDM/Stable Diffusion의 직접적 선구자가 되었으며, 벡터 양자화 코드북 개념은 DALL-E, Parti 등 이후 텍스트-이미지 모델에서도 핵심적으로 활용되었다.",
    relatedFoundations: ["vae"],
    relatedPapers: [
      { id: "ldm", fieldId: "generative", title: "LDM", relation: "successor" },
    ],
  },

  "ddim": {
    tldr: "확산 모델의 역과정을 비마르코프(non-Markovian) 과정으로 일반화하여 결정론적 샘플링을 가능하게 하고, 추론 단계를 10~50배 줄여도 높은 생성 품질을 유지하는 기법이다.",
    background: "DDPM은 뛰어난 생성 품질을 보여주었지만, 수천 스텝에 이르는 마르코프 체인의 역과정을 순차적으로 시뮬레이션해야 하므로 샘플링이 매우 느렸다(이미지 한 장에 수 분 소요). 이는 GAN의 단일 순전파에 비해 수백~수천 배 느린 것으로, 확산 모델의 실용적 적용을 심각하게 제한하는 병목이었다.",
    keyIdea: "DDIM은 DDPM과 동일한 학습 목적함수를 공유하면서도, 추론 시 비마르코프(non-Markovian) 역과정을 사용할 수 있음을 보인다. 핵심 통찰은 확산 모델의 주변 분포(marginal distribution) q(x_t|x_0)만 DDPM과 일치시키면, 중간 스텝들의 조건부 분포는 자유롭게 선택할 수 있다는 것이다. 이를 통해 (1) 전체 T 스텝의 부분 집합 S만 사용하는 서브시퀀스 샘플링이 가능하고, (2) 노이즈를 0으로 설정하면 완전히 결정론적인 역과정을 얻어, 잠재 코드와 생성 이미지 간 일대일 대응(encoding-decoding)이 가능해진다. 즉, 동일한 잠재 코드에서 항상 같은 이미지가 생성되는 일관성을 확보한다.",
    method: "DDPM과 동일한 노이즈 예측 네트워크 ε_θ를 사용하되, 역과정의 업데이트 공식을 σ(분산 파라미터)를 포함한 일반화된 형태로 변경한다. σ=0이면 결정론적 DDIM, σ가 DDPM 값이면 원래 DDPM을 복원한다. 추론 시 {1, 2, ..., T}의 부분 집합 τ(예: 50스텝)만 선택하여 샘플링하며, 각 스텝에서 예측된 노이즈로 x_0을 추정한 뒤 다음 타임스텝의 x_τ를 계산한다.",
    results: "CIFAR-10과 CelebA에서 DDPM 대비 10~50배 적은 스텝으로 동등하거나 더 나은 FID를 달성했다. 50스텝 DDIM이 1000스텝 DDPM과 유사한 품질을 보였으며, 결정론적 샘플링 덕분에 의미 있는 잠재 공간 보간(interpolation)이 가능함을 시연했다.",
    impact: "확산 모델의 느린 추론이라는 핵심 병목을 크게 완화하여 실용적 적용을 가능하게 했다. 결정론적 매핑은 이미지 편집(SDEdit), 반전(inversion), 보간 등 다양한 응용의 기반이 되었다. PNDM, DPM-Solver, PLMS 등 이후 빠른 샘플러 연구의 직접적 기반이 되었으며, LDM/Stable Diffusion에서도 핵심 샘플링 기법으로 사용된다.",
    relatedFoundations: ["ddpm"],
    relatedPapers: [
      { id: "ldm", fieldId: "generative", title: "LDM", relation: "successor" },
      { id: "classifier-free-guidance", fieldId: "generative", title: "Classifier-Free Guidance", relation: "related" },
    ],
  },

  "classifier-free-guidance": {
    tldr: "학습 시 조건 입력을 확률적으로 드롭아웃하고, 추론 시 조건부/비조건부 예측을 혼합하여 별도 분류기 없이도 생성 품질과 조건 부합도를 자유롭게 조절할 수 있는 기법이다.",
    background: "확산 모델에서 텍스트 등 조건에 충실한 이미지를 생성하기 위해 분류기 가이던스(classifier guidance)가 사용되었지만, 별도의 노이즈-인식(noise-aware) 분류기를 학습해야 하는 번거로움이 있었다. 이 분류기는 확산 모델과 별도로 학습되어야 하며, 생성 과정에서 분류기의 기울기가 불안정하거나 적대적 예제를 유발할 수 있는 문제도 있었다.",
    keyIdea: "Classifier-Free Guidance(CFG)는 별도 분류기 없이 확산 모델 자체만으로 가이던스 효과를 달성하는 간결한 방법을 제안한다. 학습 시 조건 입력 c를 일정 확률(예: 10~20%)로 빈 조건(null condition)으로 대체하여, 하나의 모델이 조건부 예측 ε_θ(x_t, c)와 비조건부 예측 ε_θ(x_t, ∅)를 모두 학습한다. 추론 시에는 가이던스 스케일 w를 사용하여 조건부 방향으로 비조건부 예측을 밀어낸다: ε̃ = ε_θ(x_t, ∅) + w · (ε_θ(x_t, c) - ε_θ(x_t, ∅)). w=1이면 표준 조건부 생성, w>1이면 조건 부합도가 높아지고(다양성은 감소), w<1이면 반대 효과를 준다.",
    method: "기존 조건부 확산 모델의 학습 파이프라인을 거의 변경하지 않고, 조건 드롭아웃만 추가한다. 텍스트 조건의 경우 CLIP 텍스트 임베딩을 빈 문자열 임베딩으로 대체하여 비조건부 학습을 수행한다. 추론 시 각 노이즈 제거 스텝에서 조건부와 비조건부 예측을 모두 계산하고 선형 외삽(extrapolation)으로 결합한다. 이로 인해 추론 비용이 약 2배 증가하지만, 배치 처리로 효율화가 가능하다.",
    results: "ImageNet 클래스 조건부 생성에서 classifier guidance와 동등하거나 우수한 FID-IS 트레이드오프를 달성했다. 가이던스 스케일 w의 조절만으로 다양성과 충실도 사이의 연속적 트레이드오프가 가능함을 보였으며, 별도 분류기 학습 없이 더 안정적이고 유연한 결과를 얻었다.",
    impact: "현대 확산 모델의 사실상 표준 가이던스 기법이 되었다. DALL-E 2, Stable Diffusion, Imagen, Midjourney 등 거의 모든 텍스트-이미지 확산 모델이 CFG를 핵심 구성 요소로 사용한다. 가이던스 스케일은 사용자가 생성 결과를 직관적으로 제어할 수 있는 중요한 하이퍼파라미터가 되었으며, 텍스트 이외에도 이미지, 오디오 등 다양한 조건부 생성에 범용적으로 적용되고 있다.",
    relatedFoundations: ["ddpm"],
    relatedPapers: [
      { id: "ldm", fieldId: "generative", title: "LDM", relation: "related" },
      { id: "dalle2", fieldId: "generative", title: "DALL-E 2", relation: "related" },
    ],
  },

  "consistency-models": {
    tldr: "확산 궤적 위의 임의의 점을 궤적의 시작점(깨끗한 이미지)으로 직접 매핑하는 일관성 함수를 학습하여, 단일 스텝만으로도 고품질 이미지를 생성할 수 있는 모델이다.",
    background: "확산 모델은 뛰어난 생성 품질을 보이지만, 수십~수백 스텝의 반복적 노이즈 제거가 필요해 생성 속도가 느렸다. DDIM, DPM-Solver 등이 스텝 수를 줄였지만 여전히 10스텝 이상이 필요했으며, 증류(distillation) 기반 방법은 사전학습된 확산 모델에 의존하는 한계가 있었다. 단일 스텝으로 GAN에 필적하는 속도와 확산 모델 수준의 품질을 동시에 달성하는 것이 이상적 목표였다.",
    keyIdea: "Consistency Models는 확산 과정의 확률 미분 방정식(PDE) 궤적에서 핵심적 성질을 활용한다: 같은 궤적 위의 모든 점은 동일한 시작점(깨끗한 데이터)에 대응한다. 이 '자기 일관성(self-consistency)' 성질을 만족하는 함수 f를 학습하면, 임의의 노이즈 수준에서 시작해 한 번의 함수 평가만으로 깨끗한 이미지를 직접 얻을 수 있다. 학습 방법은 두 가지가 있다: (1) Consistency Distillation(CD)은 사전학습된 확산 모델의 ODE 궤적에서 인접한 두 점이 같은 출력을 생성하도록 학습하고, (2) Consistency Training(CT)은 사전학습 없이 직접 일관성 함수를 학습한다. 추가 스텝을 사용하면 품질을 점진적으로 높일 수도 있다.",
    method: "일관성 함수 f_θ(x_t, t)는 경계 조건 f(x_ε, ε) = x_ε를 만족하도록 파라미터화한다. CD에서는 ODE 솔버로 x_{t_{n+1}}에서 x_{t_n}으로의 한 스텝을 수행한 뒤, f_θ(x_{t_{n+1}}, t_{n+1}) ≈ f_{θ^-}(x_{t_n}, t_n)이 되도록 학습한다(θ^-는 EMA). CT에서는 확산 과정의 점수 함수를 추정하여 유사한 목적으로 학습하되 사전학습 모델이 불필요하다. 학습 스케줄은 점진적으로 인접 타임스텝 간격을 줄이는 커리큘럼을 사용한다.",
    results: "CIFAR-10에서 단일 스텝 생성으로 FID 3.55를 달성했으며, 이는 당시 단일 스텝 생성 모델 중 최고 수준이었다. 2스텝으로는 2.93 FID까지 개선되었다. ImageNet 64x64에서도 경쟁력 있는 결과를 보였으며, Consistency Training이 증류 없이도 합리적인 성능을 달성함을 입증했다.",
    impact: "확산 모델의 반복적 샘플링이라는 근본적 한계를 극복하는 새로운 방향을 제시했다. 단일 스텝 또는 소수 스텝 생성이라는 목표에 가장 근접한 접근법으로, 실시간 이미지 생성 응용의 가능성을 열었다. Improved Consistency Training, Latent Consistency Models 등으로 빠르게 발전하며, Stable Diffusion의 실시간 생성(LCM-LoRA 등)에 직접적으로 기여하고 있다. 확산 모델과 GAN의 장점을 결합하려는 연구 방향의 중요한 이정표이다.",
    relatedFoundations: ["ddpm"],
    relatedPapers: [
      { id: "ddim", fieldId: "generative", title: "DDIM", relation: "prior" },
      { id: "flow-matching", fieldId: "generative", title: "Flow Matching", relation: "related" },
    ],
  },
};
