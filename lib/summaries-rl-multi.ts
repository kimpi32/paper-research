import type { PaperSummary } from "./paper-summaries";

export const rlMultiSummaries: Record<string, PaperSummary> = {
  "dqn": {
    tldr: "딥 뉴럴 네트워크와 경험 리플레이를 결합한 Deep Q-Network(DQN)을 제안하여, 에이전트가 원시 픽셀 입력만으로 Atari 게임에서 인간 수준의 성능을 달성한 최초의 딥 강화학습 알고리즘이다.",
    background: "기존 강화학습은 상태 공간이 크거나 고차원 입력(예: 이미지)을 다루는 데 한계가 있었다. 비선형 함수 근사기인 딥 뉴럴 네트워크를 Q-learning에 직접 적용하면 학습이 불안정해지는 문제가 잘 알려져 있었다. 이러한 불안정성을 해결하고 고차원 감각 입력에서 직접 정책을 학습하는 것이 핵심 과제였다.",
    keyIdea: "DQN은 CNN을 함수 근사기로 사용하여 Q-value를 추정하며, 두 가지 핵심 기법으로 학습 안정성을 확보한다. 첫째, 경험 리플레이(Experience Replay)를 통해 과거 전이(transition)를 버퍼에 저장하고 무작위로 샘플링하여 데이터 간 상관관계를 깨뜨린다. 둘째, 타겟 네트워크(Target Network)를 별도로 유지하여 일정 주기마다 업데이트함으로써 학습 타겟의 변동을 줄인다. 이 두 기법의 조합으로 딥 뉴럴 네트워크를 사용한 Q-learning이 안정적으로 수렴할 수 있게 되었다. 하나의 동일한 아키텍처와 하이퍼파라미터로 49개의 서로 다른 Atari 게임을 학습할 수 있다는 점에서 범용성을 입증했다.",
    method: "에이전트는 4프레임의 스택된 그레이스케일 이미지를 입력으로 받아 3개의 컨볼루션 레이어와 2개의 완전연결 레이어를 거쳐 각 행동의 Q-value를 출력한다. 100만 개의 전이를 저장하는 리플레이 버퍼에서 미니배치를 샘플링하여 학습하며, 타겟 네트워크는 매 10,000 스텝마다 동기화된다. 탐험을 위해 epsilon-greedy 전략을 사용하여 epsilon을 점진적으로 감소시킨다.",
    results: "49개 Atari 게임 중 과반수에서 기존 최고 성능을 크게 상회했으며, 여러 게임에서 인간 전문가 수준의 성능을 달성했다. 동일한 네트워크 구조, 학습 알고리즘, 하이퍼파라미터를 모든 게임에 적용하여 범용적 학습 능력을 보여주었다.",
    impact: "DQN은 딥러닝과 강화학습의 결합이 실제로 가능하다는 것을 증명하며 딥 강화학습(Deep RL) 분야를 본격적으로 개척했다. 이후 Double DQN, Dueling DQN, Rainbow 등 수많은 후속 연구의 기반이 되었으며, AlphaGo를 비롯한 게임 AI와 PPO 같은 정책 기반 알고리즘 발전에도 직접적인 영향을 미쳤다. Nature(2015)에 발표되어 AI 연구의 새로운 시대를 열었다.",
    relatedFoundations: ["backpropagation", "lstm"],
    relatedPapers: [
      { id: "alphago", fieldId: "rl", title: "AlphaGo", relation: "successor" },
      { id: "ppo", fieldId: "rl", title: "PPO", relation: "successor" },
    ],
  },

  "alphago": {
    tldr: "딥 뉴럴 네트워크(정책 네트워크 + 가치 네트워크)와 몬테카를로 트리 탐색(MCTS)을 결합하여 바둑에서 인간 세계 챔피언을 최초로 이긴 AI 시스템이다.",
    background: "바둑은 약 10^170에 달하는 거대한 상태 공간과 복잡한 전략적 깊이로 인해 전통적인 게임 AI 기법으로는 정복이 불가능하다고 여겨졌다. 체스와 달리 간단한 평가 함수를 설계하기 어렵고, 브루트포스 탐색만으로는 의미 있는 수를 찾을 수 없었다. 수십 년간 AI 연구자들의 도전에도 바둑은 아마추어 수준에 머물러 있었다.",
    keyIdea: "AlphaGo는 세 가지 핵심 구성요소를 결합한다. 정책 네트워크(Policy Network)는 프로 기사의 기보로 지도학습한 후 자기 대전(self-play)으로 강화학습하여 유망한 수를 예측한다. 가치 네트워크(Value Network)는 현재 국면의 승률을 평가한다. 이 두 네트워크를 MCTS와 결합하여, 정책 네트워크가 탐색 트리의 가지를 선택적으로 확장하고 가치 네트워크가 리프 노드를 평가함으로써 효율적인 탐색을 수행한다. 자기 대전 강화학습을 통해 인간 기보를 넘어서는 수준으로 발전할 수 있었다.",
    method: "13층 CNN 기반의 정책 네트워크를 약 3천만 개의 프로 기보 포지션으로 지도학습한 후, 이전 버전의 자기 자신과 대전하는 REINFORCE 알고리즘으로 강화학습한다. 가치 네트워크는 자기 대전에서 생성된 포지션-결과 쌍으로 학습한다. MCTS에서 각 시뮬레이션은 정책 네트워크의 사전 확률로 가이드되며, 리프 노드는 가치 네트워크 평가와 빠른 롤아웃 정책의 결과를 혼합하여 평가한다.",
    results: "2015년 10월, 유럽 챔피언 판 후이를 5:0으로 이겼으며, 2016년 3월에는 전설적인 바둑 기사 이세돌을 4:1로 이기며 역사적인 순간을 만들었다. 분산 버전의 AlphaGo는 1,202개의 CPU와 176개의 GPU를 사용하여 탐색을 수행했다.",
    impact: "AlphaGo는 AI가 인간의 직관과 전략적 사고가 필요한 영역에서도 초인적 성능을 달성할 수 있음을 보여주었다. 이 성과는 전 세계적으로 AI에 대한 관심을 폭발적으로 증가시켰으며, 후속작인 AlphaGo Zero, AlphaZero, MuZero로 이어지며 자기 대전 학습과 범용 게임 AI 연구의 새로운 패러다임을 열었다. 과학적 발견과 최적화 문제에도 유사한 접근법이 적용되는 계기가 되었다.",
    relatedFoundations: ["resnet"],
    relatedPapers: [
      { id: "dqn", fieldId: "rl", title: "DQN", relation: "prior" },
      { id: "muzero", fieldId: "rl", title: "MuZero", relation: "successor" },
    ],
  },

  "ppo": {
    tldr: "클리핑된 대리 목적함수(clipped surrogate objective)를 사용하여 정책 경사(policy gradient) 업데이트의 안정성을 확보한 범용 강화학습 알고리즘으로, 구현이 간단하면서도 우수한 성능을 보여 사실상의 표준 RL 알고리즘이 되었다.",
    background: "정책 경사(policy gradient) 방법은 연속 행동 공간과 복잡한 환경에서 유연하게 적용 가능하지만, 업데이트 스텝 크기에 민감하여 너무 크면 성능이 급격히 저하되고 너무 작으면 학습이 느려지는 문제가 있었다. TRPO(Trust Region Policy Optimization)는 이를 KL 발산 제약으로 해결했지만, 구현이 복잡하고 2차 최적화가 필요하여 실용성이 떨어졌다.",
    keyIdea: "PPO는 TRPO의 핵심 아이디어인 '정책 업데이트 크기 제한'을 훨씬 간단한 방식으로 구현한다. 확률 비율(probability ratio) r(θ) = π_θ(a|s) / π_θ_old(a|s)을 계산한 후, 이를 [1-ε, 1+ε] 범위로 클리핑하여 목적함수에 사용한다. 이렇게 하면 정책이 한 번에 너무 크게 변하는 것을 방지하면서도 1차 최적화(SGD)만으로 학습이 가능하다. 클리핑은 어드밴티지가 양수일 때는 비율의 상한을, 음수일 때는 하한을 적용하여 과도한 업데이트를 양방향으로 억제한다. 추가적으로 여러 에폭에 걸쳐 동일한 데이터를 재사용할 수 있어 샘플 효율성도 개선된다.",
    method: "병렬로 여러 환경을 실행하여 경험을 수집한 후, 수집된 데이터로 여러 에폭의 미니배치 SGD를 수행한다. 목적함수는 클리핑된 정책 목적, 가치 함수 손실, 엔트로피 보너스의 가중합으로 구성된다. GAE(Generalized Advantage Estimation)를 사용하여 어드밴티지를 추정하며, Actor-Critic 아키텍처에서 정책과 가치 함수가 네트워크 파라미터를 공유할 수 있다.",
    results: "Atari 게임, MuJoCo 로보틱스 시뮬레이션 등 다양한 벤치마크에서 TRPO, A2C, CEM 등 기존 알고리즘과 동등하거나 우수한 성능을 보였다. 특히 구현의 단순성과 튜닝의 용이성에서 큰 장점을 보여, 복잡한 제약 조건 없이도 안정적인 학습이 가능함을 입증했다.",
    impact: "PPO는 강화학습에서 사실상의 표준(de facto standard) 알고리즘이 되었다. OpenAI Five(도타2), 로보틱스, 그리고 특히 RLHF(Reinforcement Learning from Human Feedback)에서 LLM을 인간 선호도에 맞게 미세조정하는 핵심 알고리즘으로 채택되어 ChatGPT 등의 개발에 결정적 역할을 했다. 단순함과 범용성의 조합으로 학계와 산업계 모두에서 가장 널리 사용되는 RL 알고리즘이다.",
    relatedFoundations: ["backpropagation"],
    relatedPapers: [
      { id: "dqn", fieldId: "rl", title: "DQN", relation: "prior" },
      { id: "rlhf", fieldId: "safety", title: "RLHF", relation: "successor" },
    ],
  },

  "muzero": {
    tldr: "환경의 규칙을 전혀 모른 채 내부 역학 모델을 학습하여 계획(planning)을 수행함으로써, 바둑, 체스, 쇼기, Atari 게임 등 다양한 도메인에서 초인적 성능을 달성한 범용 모델 기반 강화학습 알고리즘이다.",
    background: "AlphaZero는 바둑, 체스, 쇼기에서 초인적 성능을 보였지만, 완벽한 환경 시뮬레이터(게임 규칙)가 필요하다는 제약이 있었다. 반면 모델 없이 학습하는 DQN 같은 알고리즘은 Atari를 풀 수 있지만, 계획(planning) 능력이 부족하여 성능의 한계가 있었다. 환경 모델을 직접 학습하는 기존 모델 기반 RL은 픽셀 수준의 예측이 어려워 복잡한 환경에서 잘 작동하지 않았다.",
    keyIdea: "MuZero의 핵심 통찰은 환경의 모든 것을 재현할 필요 없이, 계획에 필요한 정보만 예측하면 된다는 것이다. 세 가지 학습된 함수를 사용한다: (1) 표현 함수(representation function)가 관측을 숨겨진 상태(hidden state)로 인코딩하고, (2) 역학 함수(dynamics function)가 현재 상태와 행동으로부터 다음 상태와 즉시 보상을 예측하며, (3) 예측 함수(prediction function)가 상태로부터 정책과 가치를 출력한다. 이 학습된 모델 위에서 MCTS를 수행하여 행동을 선택한다. 중요한 점은 숨겨진 상태 공간이 관측 공간을 재구성하도록 학습되는 것이 아니라, 미래의 보상, 가치, 정책을 정확히 예측하도록 엔드투엔드로 학습된다는 것이다.",
    method: "학습 시 실제 환경과의 상호작용으로 얻은 궤적(trajectory)을 저장하고, 각 타임스텝에서 학습된 모델로 K 스텝의 가상 롤아웃을 수행한다. 각 롤아웃 스텝에서 예측된 보상, 가치, 정책을 실제 관측된 보상, MCTS로 개선된 정책, 부트스트래핑된 가치와 매칭하도록 학습한다. 추론 시에는 현재 관측을 숨겨진 상태로 인코딩한 후 학습된 모델 위에서 MCTS를 실행하여 행동을 결정한다.",
    results: "바둑, 체스, 쇼기에서 AlphaZero와 동등한 초인적 성능을 달성하면서도 게임 규칙을 전혀 사용하지 않았다. Atari 57개 게임에서는 기존 모델 프리 SOTA를 큰 폭으로 상회했다. 단일 알고리즘으로 완전 정보 보드게임과 시각적 비디오 게임을 동시에 마스터한 최초의 사례이다.",
    impact: "MuZero는 모델 기반 강화학습의 새로운 패러다임을 제시하여, 환경 규칙 없이도 효과적인 계획이 가능함을 증명했다. 이는 로봇 제어, 비디오 압축, 핵융합 플라즈마 제어 등 실제 환경 모델을 정확히 알 수 없는 현실 세계 문제에 적용 가능성을 열었다. 세계 모델(World Models) 연구와 맥을 같이하며, 학습된 시뮬레이터 기반 의사결정의 가능성을 확장했다.",
    relatedFoundations: ["resnet"],
    relatedPapers: [
      { id: "alphago", fieldId: "rl", title: "AlphaGo", relation: "prior" },
      { id: "world-models-ha", fieldId: "world-models", title: "World Models", relation: "related" },
    ],
  },

  "clip": {
    tldr: "4억 개의 이미지-텍스트 쌍에 대해 대조 학습(contrastive learning)을 수행하여, 별도의 학습 없이 텍스트 프롬프트만으로 다양한 시각 과제를 수행할 수 있는 제로샷 전이(zero-shot transfer) 능력을 갖춘 멀티모달 표현 모델이다.",
    background: "기존 컴퓨터 비전 모델은 ImageNet 같은 고정된 레이블 집합으로 지도학습되어, 새로운 카테고리나 과제에 적용하려면 추가 레이블링과 파인튜닝이 필요했다. 자연어 감독(natural language supervision)을 활용하면 미리 정의된 카테고리에 얽매이지 않는 유연한 시각 표현을 학습할 수 있다는 아이디어가 있었지만, 이전 시도들은 규모와 성능 면에서 한계가 있었다.",
    keyIdea: "CLIP(Contrastive Language-Image Pre-training)은 이미지 인코더와 텍스트 인코더를 동시에 학습하여 대응하는 이미지-텍스트 쌍의 임베딩은 가깝게, 대응하지 않는 쌍은 멀어지도록 대조 학습한다. 핵심은 인터넷에서 수집한 4억 개의 이미지-텍스트 쌍(WebImageText)이라는 대규모 데이터셋과, 효율적인 대조 학습 목적함수의 조합이다. 학습된 모델은 제로샷 분류 시 'a photo of a {class name}' 같은 텍스트 프롬프트를 생성하여 텍스트 임베딩과 이미지 임베딩의 유사도로 분류를 수행한다. 프롬프트 엔지니어링과 앙상블을 통해 제로샷 성능을 더욱 향상시킬 수 있다.",
    method: "이미지 인코더로 ResNet 또는 Vision Transformer(ViT)를, 텍스트 인코더로 Transformer를 사용한다. 미니배치 내에서 N개의 이미지-텍스트 쌍에 대해 N x N 유사도 행렬을 계산하고, 대각선(올바른 쌍)의 코사인 유사도를 최대화하는 대칭적 cross-entropy 손실을 사용한다. 학습 가능한 온도 파라미터로 로짓 스케일을 조절하며, 가장 큰 모델(ViT-L/14@336px)은 대규모 컴퓨팅으로 학습된다.",
    results: "30개 이상의 다양한 데이터셋에서 제로샷 평가를 수행한 결과, ImageNet에서 제로샷 CLIP이 기존 지도학습된 ResNet-50과 동등한 정확도를 달성했다. 특히 분포 이동(distribution shift)에 대한 강건성이 뛰어나, 기존 모델들이 성능이 크게 떨어지는 변형 데이터셋에서도 안정적인 성능을 유지했다.",
    impact: "CLIP은 멀티모달 AI의 근본적인 패러다임 전환을 가져왔다. 시각 표현을 자연어와 연결함으로써 제로샷 전이, 이미지 생성(DALL-E 2), 시각-언어 모델(Flamingo, LLaVA) 등 후속 연구의 핵심 구성요소가 되었다. '레이블이 아닌 언어로 시각을 감독한다'는 패러다임은 파운데이션 모델 시대를 여는 핵심 아이디어 중 하나로 자리잡았다.",
    relatedFoundations: ["transformer", "vit"],
    relatedPapers: [
      { id: "flamingo", fieldId: "multimodal", title: "Flamingo", relation: "successor" },
      { id: "llava", fieldId: "multimodal", title: "LLaVA", relation: "successor" },
      { id: "dalle2", fieldId: "generative", title: "DALL-E 2", relation: "successor" },
    ],
  },

  "flamingo": {
    tldr: "Perceiver Resampler와 게이티드 크로스 어텐션으로 사전학습된 비전 모델과 언어 모델을 연결하여, 이미지와 텍스트가 교차된 입력에서 소수 예시만으로 다양한 시각-언어 과제를 수행할 수 있는 비주얼 언어 모델이다.",
    background: "대규모 언어 모델(LLM)은 인컨텍스트 학습(in-context learning)을 통해 소수의 예시만으로 새로운 과제를 수행할 수 있는 놀라운 능력을 보여주었다. 하지만 이러한 능력을 시각적 입력을 포함한 멀티모달 과제로 확장하는 것은 여전히 도전적인 문제였다. 기존 비전-언어 모델들은 대부분 특정 과제에 맞춤 학습이 필요했으며, 퓨샷 멀티모달 학습 능력이 부족했다.",
    keyIdea: "Flamingo는 두 가지 핵심 아키텍처 혁신을 도입한다. 첫째, Perceiver Resampler가 다양한 크기의 시각 특징을 고정된 수의 시각 토큰으로 변환하여 연산 비용을 일정하게 유지한다. 둘째, 사전학습된 LLM의 각 레이어 사이에 게이티드 크로스 어텐션(gated cross-attention) 레이어를 삽입하여 시각 정보를 텍스트 생성에 주입한다. 게이트의 초기값을 0으로 설정하여 학습 초기에는 원래 LLM의 동작을 유지하면서 점진적으로 시각 정보를 통합한다. 이를 통해 이미지/비디오와 텍스트가 임의로 교차된 시퀀스를 처리할 수 있다.",
    method: "사전학습된 비전 인코더(NFNet)와 언어 모델(Chinchilla)을 동결(freeze)하고, Perceiver Resampler와 게이티드 크로스 어텐션 레이어만 학습한다. 웹에서 수집한 이미지-텍스트 인터리브 데이터, 이미지-텍스트 쌍, 비디오-텍스트 쌍을 혼합하여 다음 토큰 예측 목적함수로 학습한다. 가장 큰 모델인 Flamingo-80B는 800억 파라미터의 Chinchilla를 언어 모델 백본으로 사용한다.",
    results: "16개의 멀티모달 벤치마크 중 6개에서 제로샷 상태에서 기존 파인튜닝 SOTA를 능가했고, 4-shot으로는 더 많은 벤치마크에서 SOTA를 달성했다. VQA, 캡셔닝, 시각적 대화 등 다양한 과제에서 예시 수가 증가함에 따라 성능이 지속적으로 향상되는 강력한 퓨샷 학습 능력을 보여주었다.",
    impact: "Flamingo는 대규모 비주얼 언어 모델(VLM)의 설계 패턴을 확립했다. 사전학습된 비전 모델과 언어 모델을 효율적으로 연결하는 방식은 이후 LLaVA, BLIP-2, GPT-4V 등 수많은 멀티모달 모델의 기반이 되었다. 특히 '동결된 기반 모델 + 경량 어댑터'라는 패러다임은 멀티모달 AI 개발의 효율적인 접근법으로 자리잡았다.",
    relatedFoundations: ["transformer", "gpt3"],
    relatedPapers: [
      { id: "clip", fieldId: "multimodal", title: "CLIP", relation: "prior" },
      { id: "llava", fieldId: "multimodal", title: "LLaVA", relation: "successor" },
    ],
  },

  "llava": {
    tldr: "CLIP 비전 인코더와 대규모 언어 모델(LLM)을 간단한 프로젝션 레이어로 연결하고, GPT-4로 생성한 시각 지시 따르기 데이터로 미세조정하여 강력한 멀티모달 대화 능력을 구현한 시각 지시 튜닝 접근법이다.",
    background: "GPT-4 등 대규모 언어 모델이 텍스트 기반 지시 따르기(instruction following)에서 놀라운 성능을 보인 반면, 시각 입력을 포함한 멀티모달 지시 따르기는 아직 초기 단계였다. 기존 멀티모달 모델들은 대규모의 이미지-텍스트 쌍을 필요로 했으며, 복잡한 시각적 추론이나 대화를 위한 훈련 데이터가 부족했다.",
    keyIdea: "LLaVA(Large Language and Vision Assistant)는 두 가지 핵심 기여를 한다. 첫째, GPT-4를 활용하여 기존 이미지 캡션과 바운딩 박스 정보로부터 고품질의 시각 지시 따르기 데이터(대화, 상세 설명, 복잡한 추론)를 자동 생성하는 파이프라인을 제시한다. 둘째, CLIP의 사전학습된 비전 인코더(ViT-L/14)의 시각 특징을 단순한 선형 프로젝션(또는 MLP)으로 LLM의 입력 공간에 매핑하여, 시각 토큰과 텍스트 토큰을 함께 처리할 수 있게 한다. 이 간단한 아키텍처가 Flamingo 같은 복잡한 설계 없이도 효과적으로 작동한다는 것을 보여준다.",
    method: "학습은 2단계로 진행된다. 1단계(사전 정렬): LLM을 동결하고 프로젝션 레이어만 학습하여 시각-텍스트 특징 공간을 정렬한다(CC3M 필터링된 595K 이미지-텍스트 쌍). 2단계(시각 지시 튜닝): 프로젝션 레이어와 LLM을 함께 미세조정한다(GPT-4로 생성한 158K 시각 지시 따르기 데이터). Vicuna를 LLM 백본으로 사용한다.",
    results: "Science QA에서 GPT-4(텍스트 전용)를 능가하는 성능을 보였으며, 시각 대화와 상세 설명 과제에서 높은 품질을 보여주었다. GPT-4 기반 평가에서 Flamingo 등 기존 모델 대비 우수한 대화 능력을 입증했다. 비교적 적은 학습 데이터와 간단한 아키텍처로도 효과적인 멀티모달 능력을 달성할 수 있음을 보여주었다.",
    impact: "LLaVA는 시각 지시 튜닝(visual instruction tuning)이라는 새로운 학습 패러다임을 개척하여 오픈소스 멀티모달 모델 연구의 폭발적 성장을 이끌었다. 간단한 아키텍처와 데이터 생성 파이프라인의 접근성 덕분에 수많은 후속 연구(LLaVA-1.5, LLaVA-NeXT 등)와 오픈소스 VLM의 기반이 되었다. GPT-4를 교사로 활용한 데이터 생성 방식은 멀티모달 학습 데이터 구축의 효율적 방법론으로 확산되었다.",
    relatedFoundations: ["transformer", "vit"],
    relatedPapers: [
      { id: "clip", fieldId: "multimodal", title: "CLIP", relation: "prior" },
      { id: "flamingo", fieldId: "multimodal", title: "Flamingo", relation: "prior" },
    ],
  },

  "gcn": {
    tldr: "스펙트럴 그래프 이론에 기반한 그래프 컨볼루션을 1차 근사로 단순화하여, 각 노드가 1-홉 이웃의 특징을 집계하는 효율적인 레이어별 전파 규칙을 도출한 준지도 노드 분류 모델이다.",
    background: "CNN이 이미지에서, RNN이 시퀀스 데이터에서 큰 성공을 거두었지만, 소셜 네트워크, 인용 네트워크, 분자 구조 등 그래프 구조 데이터에 딥러닝을 적용하는 것은 여전히 어려웠다. 기존 스펙트럴 그래프 컨볼루션은 고유값 분해의 높은 연산 비용과 공간적 국소성 부족 등의 문제가 있었다. 준지도 학습(semi-supervised learning) 설정에서 소수의 레이블만으로 그래프의 모든 노드를 분류하는 것이 실용적으로 중요한 과제였다.",
    keyIdea: "GCN은 스펙트럴 그래프 컨볼루션에 체비셰프 다항식의 1차 근사를 적용하여, 각 레이어에서 노드가 자신과 직접 연결된 이웃의 특징을 가중 평균하는 단순한 전파 규칙을 도출한다. 전파 규칙은 H^(l+1) = σ(D̃^(-1/2) Ã D̃^(-1/2) H^(l) W^(l))로, 여기서 Ã는 자기 연결을 추가한 인접 행렬, D̃는 차수 행렬이다. 자기 연결(self-loop) 추가로 자신의 특징도 보존하면서, 정규화된 인접 행렬로 이웃 특징을 집계한다. 이 단순한 규칙을 여러 층 쌓으면 다중 홉 이웃의 정보까지 전파할 수 있다.",
    method: "입력 특징에 2-3개의 GCN 레이어를 적용하고, 최종 레이어에서 소프트맥스로 노드 분류를 수행한다. 전체 그래프의 인접 행렬을 사용한 풀배치(full-batch) 학습을 수행하며, 소수의 레이블 노드에 대한 크로스엔트로피 손실만으로 학습한다. 드롭아웃을 적용하여 과적합을 방지한다.",
    results: "Cora, Citeseer, Pubmed 인용 네트워크와 NELL 지식 그래프에서 기존 준지도 학습 방법(Label Propagation, DeepWalk, Planetoid 등)을 크게 상회했다. 적은 수의 레이블(클래스당 20개)만으로도 높은 분류 정확도를 달성했으며, 학습 속도도 매우 빨랐다.",
    impact: "GCN은 그래프 뉴럴 네트워크(GNN) 분야를 사실상 개척한 논문으로, 수천 편의 후속 연구에 영향을 미쳤다. 메시지 패싱(message passing)이라는 GNN의 핵심 프레임워크를 확립했으며, GAT, GraphSAGE 등 수많은 변형 모델의 기반이 되었다. 소셜 네트워크 분석, 추천 시스템, 약물 발견, 교통 예측 등 다양한 실세계 응용에서 그래프 기반 딥러닝의 표준 빌딩 블록이 되었다.",
    relatedFoundations: ["backpropagation"],
    relatedPapers: [
      { id: "gat", fieldId: "graph", title: "GAT", relation: "successor" },
      { id: "graphsage", fieldId: "graph", title: "GraphSAGE", relation: "successor" },
    ],
  },

  "gat": {
    tldr: "그래프의 각 노드가 이웃 노드들에 어텐션 메커니즘을 적용하여 적응적으로 가중치를 부여함으로써, 이웃의 중요도를 동적으로 학습할 수 있는 그래프 어텐션 네트워크이다.",
    background: "GCN은 이웃의 특징을 고정된 정규화 계수로 집계하여, 모든 이웃을 동등하게(차수에 의한 정규화만 적용) 취급했다. 하지만 실제 그래프에서는 이웃 노드들의 중요도가 다를 수 있으며, 이를 학습적으로 반영할 수 있는 메커니즘이 필요했다. 또한 GCN의 스펙트럴 접근법은 그래프 구조에 종속적이어서 새로운 그래프에 직접 적용하기 어려운 한계가 있었다.",
    keyIdea: "GAT(Graph Attention Network)는 Transformer의 셀프 어텐션에서 영감을 받아, 이웃 노드 쌍 사이의 어텐션 계수를 학습한다. 각 노드 쌍 (i, j)에 대해 학습 가능한 어텐션 메커니즘 a를 적용하여 가중치 α_ij를 계산하고, 이를 사용하여 이웃 특징의 가중합을 구한다. 멀티헤드 어텐션을 사용하여 여러 개의 독립적인 어텐션 헤드를 병렬로 적용하고 결과를 연결(concatenate)하거나 평균하여 표현력과 안정성을 높인다. 어텐션 계수가 이웃 노드의 특징에 의해 결정되므로, 다른 노드나 다른 그래프에서도 직접 적용 가능한 귀납적(inductive) 학습이 가능하다.",
    method: "각 레이어에서 노드 특징을 선형 변환한 후, 연결된 노드 쌍의 변환된 특징을 결합(concatenation)하여 단일 레이어 피드포워드 네트워크와 LeakyReLU를 거쳐 어텐션 로짓을 계산한다. 소프트맥스로 정규화하여 어텐션 계수를 얻고, K개의 헤드를 병렬로 실행한다. 마스크드 어텐션으로 그래프의 연결 구조를 반영한다.",
    results: "Cora, Citeseer, Pubmed 인용 네트워크 벤치마크에서 GCN 및 기타 기존 방법을 상회하거나 동등한 성능을 달성했다. PPI(Protein-Protein Interaction) 데이터셋의 귀납적 학습 설정에서도 GraphSAGE의 모든 변형을 능가하여, 귀납적 학습 능력을 입증했다.",
    impact: "GAT는 그래프 뉴럴 네트워크에 어텐션 메커니즘을 성공적으로 도입하여, GNN 설계에서 적응적 이웃 집계의 중요성을 확립했다. 이후 GATv2(정적 어텐션 한계 극복), Graph Transformer 등 어텐션 기반 GNN의 발전을 이끌었으며, 약물 발견, 추천 시스템, 자연어 처리의 구문 분석 등에서 널리 활용되고 있다.",
    relatedFoundations: ["attention-mechanism"],
    relatedPapers: [
      { id: "gcn", fieldId: "graph", title: "GCN", relation: "prior" },
      { id: "graphsage", fieldId: "graph", title: "GraphSAGE", relation: "related" },
    ],
  },

  "graphsage": {
    tldr: "이웃 노드를 샘플링하고 집계하는(sample and aggregate) 귀납적 학습 프레임워크로, 학습 시 보지 못한 새로운 노드에도 임베딩을 생성할 수 있는 확장 가능한 그래프 표현 학습 방법이다.",
    background: "GCN 등 기존 그래프 임베딩 방법들은 학습 시 전체 그래프가 필요한 변환적(transductive) 방식이어서, 학습 후 새로 추가되는 노드에 대해서는 전체를 다시 학습해야 했다. 이는 수십억 개의 노드가 지속적으로 추가되는 소셜 네트워크나 생물학적 네트워크 같은 대규모 동적 그래프에서는 실용적이지 않았다. 또한 전체 그래프를 메모리에 올려야 하는 풀배치 학습의 확장성 문제도 있었다.",
    keyIdea: "GraphSAGE(SAmple and aggreGatE)는 노드의 임베딩을 생성하는 '함수'를 학습한다는 핵심 아이디어에 기반한다. 각 노드에 대해: (1) 고정된 수의 이웃을 무작위로 샘플링하고, (2) 집계 함수(mean, LSTM, pooling)로 이웃 정보를 요약한 후, (3) 자신의 특징과 결합하여 새로운 표현을 생성한다. 이 과정을 K 홉까지 반복하며, 각 홉의 집계 함수가 학습 가능한 파라미터를 가진다. 고정 크기 이웃 샘플링 덕분에 미니배치 학습이 가능하고, 학습된 집계 함수를 새로운 노드에 직접 적용할 수 있어 귀납적(inductive) 학습이 가능하다.",
    method: "각 레이어에서 이웃 중 S개를 균일 무작위 샘플링하여 고정 크기의 연산 그래프를 구성한다. 집계 함수로 mean(평균), LSTM(순서 무작위화 후), max-pooling 중 선택하여 사용한다. 비지도 학습(인접 노드는 가까이, 비인접 노드는 멀리)과 지도 학습(크로스엔트로피) 모두 지원하며, 미니배치 SGD로 학습한다.",
    results: "Reddit 포스트 분류, PPI 단백질 기능 예측 등에서 기존 방법을 크게 상회했다. 특히 학습 시 보지 못한 노드에 대한 귀납적 성능이 뛰어났다. 전체 Reddit 그래프(23만 노드)에서 DeepWalk 대비 성능이 우수하면서 학습 시간은 크게 단축되었다. Pooling 집계 함수가 전반적으로 가장 우수한 성능을 보였다.",
    impact: "GraphSAGE는 대규모 그래프에서의 귀납적 학습과 확장 가능한 학습이라는 두 가지 핵심 문제를 해결하여, GNN의 실용적 적용 범위를 크게 넓혔다. Pinterest의 추천 시스템(PinSage)에 적용되는 등 산업 규모의 그래프 학습에 직접적인 영향을 미쳤다. 이웃 샘플링과 미니배치 학습이라는 아이디어는 이후 거의 모든 확장 가능한 GNN 방법에 채택되었다.",
    relatedFoundations: ["backpropagation"],
    relatedPapers: [
      { id: "gcn", fieldId: "graph", title: "GCN", relation: "prior" },
      { id: "gat", fieldId: "graph", title: "GAT", relation: "related" },
    ],
  },

  "saycan": {
    tldr: "대규모 언어 모델(LLM)의 세계 지식과 로봇의 어포던스(affordance) 점수를 결합하여, 자연어 지시를 물리적으로 실행 가능한 로봇 행동 계획으로 변환하는 프레임워크이다.",
    background: "대규모 언어 모델은 방대한 세계 지식과 상식적 추론 능력을 갖추고 있지만, 물리적 세계에 대한 접지(grounding)가 부족하여 '냉장고에서 음료를 가져와'라는 지시에 대해 논리적이지만 물리적으로 불가능한 계획을 생성할 수 있다. 반면 로봇 학습은 개별 기술(skill)을 잘 수행하지만, 복잡한 장기 과제를 위한 고수준 계획 능력이 부족했다.",
    keyIdea: "SayCan은 LLM이 '할 수 있다고 말하는 것(Say)'이 아닌 '실제로 할 수 있는 것(Can)'에 기반하여 행동을 선택한다. 각 단계에서 LLM은 현재까지의 맥락을 고려하여 가능한 기술(skill)들의 유용성 점수를 생성하고, 별도의 어포던스 모델(각 기술의 성공 확률을 추정하는 가치 함수)이 물리적 실행 가능성 점수를 제공한다. 두 점수를 곱하여 가장 높은 점수의 기술을 선택하고 실행한 뒤, 결과를 피드백하여 다음 단계를 반복한다. 이를 통해 LLM의 추상적 계획 능력과 로봇의 물리적 제약을 자연스럽게 통합한다.",
    method: "사전학습된 LLM(PaLM, FLAN 등)을 사용하여 각 기술에 대한 언어 점수를 계산하고, 강화학습으로 학습된 551개의 기본 기술 정책과 해당 가치 함수를 어포던스 모델로 활용한다. 실제 주방 환경의 모바일 매니퓰레이터 로봇에서 실험하며, 기술 실행 후 성공/실패 판정에는 학습된 분류기를 사용한다.",
    results: "101개의 실제 로봇 과제에서 SayCan은 계획 정확도 84%를 달성했으며, 이는 LLM만 사용한 경우(14%)나 어포던스만 사용한 경우(13%)를 크게 상회한다. '음료를 쏟았어, 도와줘' 같은 추상적이고 개방적인 지시도 적절한 행동 시퀀스(스펀지 가져오기 → 닦기 → 스펀지 치우기)로 변환할 수 있었다.",
    impact: "SayCan은 파운데이션 모델을 로봇 제어에 활용하는 새로운 패러다임을 열어, 이후 RT-2, PaLM-E, Code as Policies 등 LLM 기반 로봇 연구의 물결을 촉발했다. LLM의 언어적 추론과 물리적 실행 가능성을 분리하여 결합하는 프레임워크는 구체화된 AI(Embodied AI) 연구의 핵심 참조점이 되었다.",
    relatedFoundations: ["gpt3", "transformer"],
    relatedPapers: [
      { id: "rt2", fieldId: "robotics", title: "RT-2", relation: "successor" },
    ],
  },

  "rt2": {
    tldr: "대규모 비전-언어 모델(VLM)을 로봇 행동 데이터로 미세조정하여, 시각 관측을 텍스트 토큰 형태의 로봇 행동으로 직접 변환하는 비전-언어-행동(Vision-Language-Action) 모델이다.",
    background: "SayCan 등의 기존 접근법은 LLM을 고수준 계획기로만 활용하고 저수준 제어는 별도의 정책에 의존했다. RT-1은 로봇 데이터로 학습한 전용 모델로 실시간 제어에 성공했지만, 인터넷 규모의 지식을 활용하지 못하는 한계가 있었다. 대규모 사전학습된 VLM의 시각적 이해와 추론 능력을 저수준 로봇 제어에 직접 활용하는 것이 과제였다.",
    keyIdea: "RT-2는 로봇 행동을 텍스트 토큰으로 표현하여, VLM의 출력으로 직접 로봇을 제어할 수 있게 한다. 로봇의 연속적인 행동(팔 이동, 그리퍼 열림/닫힘 등)을 256개의 구간으로 이산화하여 정수 토큰으로 인코딩한다. 예를 들어 '1 128 91 241 5 101 127'이 팔의 7-DoF 행동을 나타낸다. PaLI-X(55B)나 PaLM-E(12B) 같은 사전학습된 VLM을 로봇 궤적 데이터로 미세조정하면, 모델이 '빨간 블록을 잡아라'라는 지시와 카메라 이미지를 입력받아 행동 토큰을 직접 생성한다. 인터넷 규모의 사전학습 덕분에 로봇 데이터에 없는 새로운 객체나 개념에 대한 제로샷 일반화가 가능하다.",
    method: "PaLI-X 또는 PaLM-E를 백본으로 사용하며, 웹 데이터와 로봇 에피소드 데이터를 함께 사용하여 미세조정한다(co-fine-tuning). 로봇 데이터는 기존 RT-1 데이터셋을 재활용하며, 행동 토큰은 기존 어휘에 추가하지 않고 숫자 토큰을 재사용한다. 추론 시 이미지와 언어 지시를 입력으로 받아 다음 행동 토큰 시퀀스를 자기회귀적으로 생성한다.",
    results: "RT-1 대비 학습 시 본 객체와 시나리오에서 동등한 성능을 유지하면서, 보지 못한 객체, 배경, 환경에 대한 일반화에서 크게 향상되었다. 특히 '테일러 스위프트 근처의 음료를 가져와' 같은 상징적 추론이 필요한 과제에서 RT-2(PaLI-X 55B)가 62%의 성공률을 보인 반면 RT-1은 0%였다.",
    impact: "RT-2는 대규모 인터넷 사전학습 지식을 로봇 제어에 직접 전이하는 것이 가능함을 증명하여, 로봇 파운데이션 모델의 가능성을 열었다. VLA(Vision-Language-Action) 모델이라는 새로운 패러다임을 확립하여 이후 RT-X, Octo, OpenVLA 등 범용 로봇 정책 연구의 방향을 제시했다. 로봇 공학과 멀티모달 AI의 융합이 가속화되는 계기가 되었다.",
    relatedFoundations: ["transformer", "vit"],
    relatedPapers: [
      { id: "saycan", fieldId: "robotics", title: "SayCan", relation: "prior" },
      { id: "llava", fieldId: "multimodal", title: "LLaVA", relation: "related" },
    ],
  },
};
