import type { PaperSummary } from "./paper-summaries";

export const newRlRobotWorldSummaries: Record<string, PaperSummary> = {
  // ===== RL Field =====
  "a3c": {
    tldr: "여러 개의 병렬 액터가 각각 독립적인 환경에서 비동기적으로 경험을 수집하고, 공유 글로벌 네트워크에 그래디언트를 전달하는 Asynchronous Advantage Actor-Critic(A3C) 알고리즘으로, 경험 리플레이 버퍼 없이도 안정적이고 효율적인 강화학습을 달성했다.",
    background: "DQN은 경험 리플레이 버퍼를 통해 데이터 간 상관관계를 깨뜨려 안정적인 학습을 가능하게 했지만, 대용량 메모리와 오프폴리시 학습이 필수적이었다. 또한 단일 에이전트가 순차적으로 환경과 상호작용하므로 학습 속도에 한계가 있었다. 온폴리시 방법은 데이터 효율이 낮고, 오프폴리시 방법은 리플레이 버퍼의 메모리 부담이 컸다.",
    keyIdea: "A3C의 핵심 통찰은 여러 액터-러너(actor-learner)를 병렬로 실행하면 각 액터가 서로 다른 상태를 탐험하므로 데이터 간 상관관계가 자연스럽게 깨진다는 것이다. 따라서 경험 리플레이 버퍼가 불필요해진다. 각 액터는 글로벌 네트워크의 파라미터를 복사하여 n-스텝 동안 환경과 상호작용한 후, 어드밴티지 함수 A(s,a) = R - V(s)를 기반으로 정책 그래디언트와 가치 함수 그래디언트를 계산하여 비동기적으로 글로벌 네트워크를 업데이트한다. 엔트로피 정규화 항을 추가하여 조기 수렴을 방지하고 탐험을 촉진한다.",
    method: "CPU 멀티스레딩을 활용하여 16개의 액터를 병렬로 실행한다. 각 스레드는 글로벌 네트워크의 사본으로 최대 t_max 스텝의 경험을 수집하고, n-스텝 리턴으로 어드밴티지를 추정하여 비동기적으로 글로벌 파라미터를 갱신한다. 정책과 가치 함수가 파라미터를 공유하는 Actor-Critic 구조를 사용하며, LSTM 레이어를 추가하여 부분 관측 문제에도 대응한다.",
    results: "Atari 게임에서 GPU 없이 멀티코어 CPU만으로 DQN의 절반 시간에 동등하거나 우수한 성능을 달성했다. 연속 행동 공간(MuJoCo), 미로 탐색(TORCS) 등 다양한 도메인에서 효과적으로 작동했으며, 학습 시간이 액터 수에 거의 선형적으로 감소함을 보였다.",
    impact: "A3C는 병렬 환경 실행을 통한 분산 강화학습의 패러다임을 확립했다. 경험 리플레이 없는 온폴리시 학습이 가능함을 보여줌으로써, 이후 PPO, IMPALA, Ape-X 등 대규모 분산 RL 알고리즘의 기반이 되었다. 특히 GPU가 아닌 CPU만으로 효율적인 학습이 가능하다는 점에서 강화학습의 접근성을 크게 높였다.",
    relatedFoundations: ["backpropagation", "lstm"],
    relatedPapers: [
      { id: "dqn", fieldId: "rl", title: "DQN", relation: "prior" },
      { id: "ppo", fieldId: "rl", title: "PPO", relation: "successor" },
    ],
  },

  "alphazero": {
    tldr: "자기 대전(self-play) 강화학습과 몬테카를로 트리 탐색(MCTS)만으로 체스, 쇼기, 바둑 세 가지 보드게임 모두에서 각 분야 최강 프로그램을 능가한 범용 게임 AI 알고리즘이다.",
    background: "AlphaGo와 AlphaGo Zero는 바둑에서 초인적 성능을 달성했지만 바둑에 특화된 설계 요소를 포함하고 있었다. 체스에서는 Stockfish가 수십 년간 축적된 인간 도메인 지식(오프닝 북, 엔드게임 테이블, 수작업 평가 함수)을 기반으로 최강의 성능을 보이고 있었다. 단일 알고리즘으로 서로 다른 규칙과 전략적 특성을 가진 여러 게임을 마스터할 수 있는지가 핵심 질문이었다.",
    keyIdea: "AlphaZero는 게임 규칙 외에 어떠한 도메인 지식도 사용하지 않는다. 단일 뉴럴 네트워크 f_θ(s) → (p, v)가 현재 보드 상태에서 행동 확률 분포 p와 승률 예측 v를 동시에 출력한다. MCTS는 이 네트워크를 가이드로 사용하여 탐색 트리를 확장하며, 탐색 결과로 개선된 정책을 다시 네트워크 학습의 타겟으로 사용한다. 자기 대전으로 생성된 게임 데이터만으로 학습하며, 동일한 아키텍처, 하이퍼파라미터, 알고리즘으로 세 게임을 모두 학습한다. AlphaGo Zero와 달리 대칭 증강을 제거하고, 게임별 수정 없이 동작하도록 일반화했다.",
    method: "ResNet 기반의 뉴럴 네트워크(19개 또는 39개 잔차 블록)를 사용하며, 5,000개의 TPU로 자기 대전을 수행하여 학습 데이터를 생성한다. 각 수(move)마다 800번의 MCTS 시뮬레이션을 실행하고, MCTS의 방문 횟수 분포를 정책 타겟으로, 게임 결과를 가치 타겟으로 사용하여 네트워크를 업데이트한다. 바둑은 약 70만 스텝, 체스는 약 30만 스텝, 쇼기는 약 11만 스텝 학습한다.",
    results: "체스에서 Stockfish를 압도적으로 이겼으며(28승 72무 0패), 쇼기에서 elmo를, 바둑에서 AlphaGo Zero 3일차 버전을 각각 능가했다. 세 게임 모두 수 시간의 자기 대전 학습만으로 초인적 수준에 도달했으며, 체스에서는 인간 전문가가 '외계인 같은 직관적 플레이'라고 평가한 독창적인 전략을 발견했다.",
    impact: "AlphaZero는 도메인 지식 없는 단일 알고리즘이 복수의 완전 정보 게임에서 최정상에 도달할 수 있음을 증명하며, 범용 AI 알고리즘의 가능성을 실증했다. 이후 MuZero(환경 모델 학습), AlphaStar(불완전 정보 게임) 등으로 확장되었으며, 체스 엔진 Leela Chess Zero 등 오픈소스 프로젝트에도 직접적 영향을 미쳤다. 과학적 발견(단백질 구조 예측, 수학 정리 증명)에 자기 대전 학습을 적용하는 연구에도 영감을 주었다.",
    relatedFoundations: ["resnet"],
    relatedPapers: [
      { id: "alphago", fieldId: "rl", title: "AlphaGo", relation: "prior" },
      { id: "muzero", fieldId: "rl", title: "MuZero", relation: "successor" },
    ],
  },

  "sac": {
    tldr: "정책의 엔트로피를 보상에 포함시키는 최대 엔트로피(maximum entropy) 프레임워크 기반의 오프폴리시 액터-크리틱 알고리즘으로, 확률적 정책과 자동 온도 조절을 통해 연속 행동 공간에서 안정적이고 샘플 효율적인 학습을 달성했다.",
    background: "연속 행동 공간의 강화학습에서 기존 온폴리시 방법(PPO, A3C)은 샘플 효율이 낮고, 오프폴리시 방법(DDPG, TD3)은 결정적 정책을 사용하여 탐험이 부족하고 하이퍼파라미터에 민감했다. 특히 결정적 정책은 다봉(multimodal) 행동 분포를 표현하지 못하고, 수동으로 탐험 노이즈를 추가해야 하는 한계가 있었다. 안정성과 샘플 효율을 동시에 달성하면서도 탐험-활용 균형을 자동으로 관리하는 알고리즘이 필요했다.",
    keyIdea: "SAC(Soft Actor-Critic)는 기대 보상뿐 아니라 정책의 엔트로피도 함께 최대화하는 최대 엔트로피 강화학습 프레임워크를 기반으로 한다. 목적함수는 J(π) = Σ E[r(s,a) + αH(π(·|s))]로, α가 보상과 엔트로피의 균형을 조절하는 온도 파라미터이다. 확률적 정책(가우시안)을 사용하여 자연스러운 탐험이 이루어지며, 리파라미터화 트릭으로 정책 그래디언트를 효율적으로 계산한다. 핵심 기여 중 하나인 자동 온도 조절은 목표 엔트로피를 설정하면 α가 자동으로 조정되어, 도메인별 수동 튜닝을 제거한다.",
    method: "두 개의 Q-네트워크(Double Q)와 하나의 정책 네트워크, 그리고 학습 가능한 온도 파라미터 α를 사용한다. 리플레이 버퍼에서 미니배치를 샘플링하여 소프트 벨만 방정식에 기반한 Q-함수 손실, 정책 네트워크의 KL 발산 최소화 손실, 그리고 α의 제약 최적화 손실을 동시에 업데이트한다. 타겟 네트워크는 지수 이동 평균으로 소프트 업데이트한다.",
    results: "MuJoCo 벤치마크(HalfCheetah, Ant, Humanoid 등)에서 DDPG, TD3, PPO를 포함한 기존 모든 알고리즘을 일관되게 상회했다. 특히 복잡한 보행 과제(Humanoid)에서 기존 방법 대비 월등한 성능을 보였으며, 하이퍼파라미터 민감도가 크게 낮아 다양한 도메인에서 동일한 설정으로 잘 작동했다.",
    impact: "SAC는 연속 제어 영역에서 사실상의 표준 알고리즘이 되어, 로보틱스 시뮬레이션 및 실제 로봇 학습에서 가장 널리 사용되는 방법 중 하나가 되었다. 최대 엔트로피 프레임워크는 탐험-활용 트레이드오프를 원칙적으로 해결하는 이론적 기반을 제공했으며, 이후 offline RL과 Decision Transformer 등 시퀀스 기반 RL 연구에도 영향을 미쳤다.",
    relatedFoundations: ["backpropagation"],
    relatedPapers: [
      { id: "ppo", fieldId: "rl", title: "PPO", relation: "related" },
      { id: "decision-transformer", fieldId: "rl", title: "Decision Transformer", relation: "successor" },
    ],
  },

  "alphastar": {
    tldr: "다중 에이전트 강화학습과 리그 훈련(league training)을 통해 실시간 전략 게임 StarCraft II에서 인간 그랜드마스터 수준에 도달한 최초의 AI 시스템이다.",
    background: "StarCraft II는 불완전 정보, 거대한 행동 공간(~10^26 가능 행동), 장기 전략 수립, 실시간 의사결정이 요구되는 극도로 복잡한 게임으로, 바둑보다 훨씬 더 도전적인 AI 벤치마크로 여겨졌다. 불완전 정보로 인한 게임 이론적 복잡성, 다양한 전략 간의 비이행적(non-transitive) 관계(가위바위보와 유사), 그리고 수만 프레임에 걸친 장기 계획의 필요성이 핵심 난제였다.",
    keyIdea: "AlphaStar의 핵심 혁신은 리그 훈련(league training) 시스템이다. 단순한 자기 대전은 전략 간 순환적 우열 관계 때문에 특정 전략에 과적합될 수 있다. 리그 훈련은 메인 에이전트(main agent), 메인 익스플로이터(main exploiter), 리그 익스플로이터(league exploiter) 세 유형의 에이전트를 동시에 훈련한다. 메인 에이전트는 범용 전략을, 메인 익스플로이터는 메인 에이전트의 약점을, 리그 익스플로이터는 리그 전체의 약점을 공략한다. 과거 에이전트들의 스냅샷을 리그에 보존하여 전략적 다양성을 유지하며, 게임 이론의 내시 균형에 근접하도록 설계되었다.",
    method: "Transformer와 LSTM을 결합한 딥 뉴럴 네트워크가 미니맵, 유닛 정보, 스칼라 특성을 입력으로 받아 자기회귀적으로 행동을 생성한다. 지도학습으로 인간 리플레이에서 초기화한 후 리그 훈련으로 강화학습한다. 각 에이전트는 16개의 TPU v3로 44일간 훈련되며, 총 약 200년 분량의 게임 경험을 소화한다. 포인터 네트워크로 유닛 선택을 처리하고, 스칼라 값과 임베딩의 조합으로 복잡한 행동 공간을 표현한다.",
    results: "Battle.net 래더에서 세 종족(프로토스, 테란, 저그) 모두에서 그랜드마스터(상위 0.2%) 수준에 도달했다. 프로 게이머 Serral과 TLO를 상대로 10-1로 승리했으며, MaNa를 상대로도 5-0으로 승리했다. 인간과 동일한 APM(분당 행동 수) 제한과 카메라 제약 하에서도 이 성과를 달성했다.",
    impact: "AlphaStar는 불완전 정보의 복잡한 다중 에이전트 환경에서 AI가 인간 최고 수준에 도달할 수 있음을 증명했다. 리그 훈련은 다중 에이전트 학습에서 전략적 다양성을 유지하는 효과적 방법론으로 확립되었으며, 이후 OpenAI Five(도타2), Cicero(디플로매시) 등 게임 AI 연구에 영향을 미쳤다. 실시간 의사결정과 장기 전략이 동시에 필요한 현실 세계 문제에 대한 적용 가능성도 시사했다.",
    relatedFoundations: ["lstm", "transformer"],
    relatedPapers: [
      { id: "alphazero", fieldId: "rl", title: "AlphaZero", relation: "prior" },
      { id: "muzero", fieldId: "rl", title: "MuZero", relation: "related" },
    ],
  },

  "decision-transformer": {
    tldr: "강화학습 문제를 시퀀스 모델링 문제로 재구성하여, Transformer가 (리턴, 상태, 행동)의 시퀀스를 조건부로 생성함으로써 기존 RL 알고리즘 없이도 오프라인 RL 과제를 수행할 수 있음을 보여주었다.",
    background: "기존 오프라인 강화학습(Offline RL)은 사전 수집된 데이터셋에서 최적 정책을 학습해야 하는데, 분포 이동(distribution shift) 문제로 인해 과대추정된 Q-값이 불안정한 학습을 초래하는 근본적 어려움이 있었다. CQL, IQL 등의 보수적 알고리즘이 제안되었지만 복잡한 제약 조건이 필요했다. 한편 GPT 시리즈의 성공은 시퀀스 예측의 놀라운 잠재력을 보여주었다.",
    keyIdea: "Decision Transformer는 강화학습을 '원하는 리턴을 달성하는 행동 시퀀스를 생성하는 조건부 시퀀스 모델링'으로 재정의한다. 에이전트의 궤적을 (R̂₁, s₁, a₁, R̂₂, s₂, a₂, ...) 형태의 시퀀스로 표현하되, R̂ₜ는 해당 시점 이후의 목표 리턴(return-to-go)이다. GPT 아키텍처의 인과적(causal) Transformer가 이 시퀀스에서 다음 행동을 자기회귀적으로 예측한다. 테스트 시 높은 목표 리턴을 조건으로 주면 모델이 해당 리턴을 달성하는 행동을 생성한다. 벨만 방정식, 정책 그래디언트, 행동 복제 같은 기존 RL 프레임워크를 전혀 사용하지 않는다.",
    method: "오프라인 데이터셋의 궤적을 (return-to-go, 상태, 행동) 트리플렛 시퀀스로 변환한다. 각 모달리티에 대해 선형 임베딩 레이어를 적용하고, 타임스텝 임베딩을 더한 후 GPT-2 스타일의 Transformer에 입력한다. 최근 K 타임스텝의 컨텍스트를 유지하며, 행동 예측에 대한 교차 엔트로피(이산 행동) 또는 MSE(연속 행동) 손실만으로 학습한다.",
    results: "Atari(이산 행동)와 D4RL(연속 행동) 벤치마크에서 CQL 등 기존 오프라인 RL 알고리즘과 동등하거나 우수한 성능을 달성했다. 특히 보상이 희소한(sparse reward) 환경에서 기존 TD 학습 기반 방법보다 크게 우수했으며, 이는 장기 신용 할당(long-term credit assignment)에서 시퀀스 모델링의 이점을 보여준다.",
    impact: "Decision Transformer는 RL과 시퀀스 모델링의 경계를 허물어, 파운데이션 모델 시대에 RL 문제를 접근하는 새로운 패러다임을 제시했다. 이후 Trajectory Transformer, GATO, RT-2 등 시퀀스 모델링 기반 의사결정 연구의 폭발적 성장을 이끌었으며, LLM의 인컨텍스트 학습 능력을 의사결정에 활용하는 연구 방향을 열었다.",
    relatedFoundations: ["transformer"],
    relatedPapers: [
      { id: "gpt3", fieldId: "llm", title: "GPT-3", relation: "related" },
      { id: "dreamerv3", fieldId: "rl", title: "DreamerV3", relation: "related" },
    ],
  },

  "dreamerv3": {
    tldr: "심볼릭 이산 표현, 하이퍼파라미터 고정, 그리고 안정화된 학습 기법을 결합한 범용 월드 모델 강화학습 알고리즘으로, 하이퍼파라미터 수정 없이 150개 이상의 다양한 도메인에서 작동하며 최초로 마인크래프트에서 다이아몬드를 획득했다.",
    background: "모델 기반 강화학습(Model-based RL)은 환경 모델을 학습하여 상상(imagination) 속에서 정책을 학습함으로써 샘플 효율을 높이는 접근법이다. DreamerV1, V2가 Atari 등에서 성과를 보였지만, 도메인마다 하이퍼파라미터 조정이 필요했으며 보상의 스케일, 관측의 차원, 행동 공간의 유형이 크게 다른 도메인 간 전환이 어려웠다. 마인크래프트 다이아몬드 획득은 희소 보상, 장기 계획, 다양한 서브태스크를 요구하는 극히 어려운 벤치마크였다.",
    keyIdea: "DreamerV3는 세 가지 핵심 혁신으로 범용성을 달성한다. 첫째, 이산 잠재 표현(categorical latent)을 사용하여 세계 모델이 다양한 유형의 환경을 일관되게 표현할 수 있다. 둘째, symlog 예측(symlog predictions)으로 보상과 가치의 스케일 문제를 해결한다. symlog(x) = sign(x)·log(|x|+1) 변환을 통해 크기가 매우 다른 보상을 통일적으로 처리한다. 셋째, 정책 학습 시 리턴을 백분위 기반으로 정규화하여 도메인에 무관한 목표 스케일을 유지한다. 이 조합으로 단일 설정이 모든 도메인에서 작동한다.",
    method: "RSSM(Recurrent State-Space Model)을 기반으로 한 세계 모델이 관측을 이산 잠재 상태로 인코딩하고, GRU 기반 시퀀스 모델이 전이를 예측한다. 세계 모델의 상상 속에서 Actor-Critic을 학습하며, 액터는 symlog 변환된 리턴의 백분위 정규화된 값을 최대화한다. 가치 함수는 이산 회귀(discrete regression)로 학습하여 분포적 가치 추정의 이점을 얻는다.",
    results: "Atari 200M, DMC Vision, DMC Proprio, BSuite, Crafter, Minecraft 등 150개 이상의 과제에서 단일 하이퍼파라미터 설정으로 평가했다. 대부분의 도메인에서 도메인별 튜닝된 기존 최고 알고리즘과 동등하거나 우수한 성능을 보였다. 마인크래프트에서는 기존에 어떤 RL 에이전트도 달성하지 못한 다이아몬드 획득을 최초로 성공시켰다.",
    impact: "DreamerV3는 '단일 알고리즘, 단일 하이퍼파라미터로 모든 도메인'이라는 범용 RL의 이상에 가장 가까이 다가간 성과로, 모델 기반 RL의 실용성을 획기적으로 증명했다. 이후 대규모 세계 모델과 범용 에이전트 연구의 핵심 참조점이 되었으며, 마인크래프트 다이아몬드 획득은 장기 계획 능력의 중요한 이정표가 되었다.",
    relatedFoundations: ["vae", "lstm"],
    relatedPapers: [
      { id: "muzero", fieldId: "rl", title: "MuZero", relation: "prior" },
      { id: "world-models-ha", fieldId: "world-models", title: "World Models", relation: "prior" },
    ],
  },

  // ===== Robotics Field =====
  "bcz": {
    tldr: "언어로 조건화된 모방 학습(language-conditioned imitation learning)을 통해 학습 시 본 적 없는 새로운 로봇 조작 과제에 제로샷으로 일반화할 수 있는 BC-Z(Behavioral Cloning - Zero-shot) 프레임워크이다.",
    background: "기존 로봇 모방 학습은 특정 과제별로 시연 데이터를 수집하고 별도의 정책을 학습해야 했다. 새로운 과제가 추가될 때마다 추가 시연이 필요하며, 학습 시 본 적 없는 과제에는 적용할 수 없었다. 자연어를 과제 명세(task specification)로 활용하면 과제 간 공유된 구조를 학습하여 새로운 과제로의 일반화가 가능할 것이라는 가설이 있었으나, 이를 실제 로봇 환경에서 대규모로 검증한 연구는 부족했다.",
    keyIdea: "BC-Z는 대규모 다과제 시연 데이터셋(100개 이상의 과제, 총 약 36,000 에피소드)을 수집하고, 자연어 지시와 비디오 시연 모두를 과제 조건으로 사용할 수 있는 통합 정책을 학습한다. 정책 네트워크는 현재 이미지 관측과 언어 임베딩(또는 시연 비디오 임베딩)을 입력으로 받아 로봇 행동을 출력한다. 핵심 통찰은 충분히 다양한 과제에서 학습하면, 정책이 '언어가 묘사하는 의미'를 이해하여 새로운 조합의 객체-동사에 대해서도 적절한 행동을 생성할 수 있다는 것이다.",
    method: "실제 로봇(UR5e 팔)으로 100개 이상의 과제에 대한 원격 조작 시연을 수집한다. 각 시연에는 자연어 지시가 태깅된다. 정책은 ResNet 기반 이미지 인코더와 사전학습된 언어 모델 임베딩을 결합하여 연속 행동을 예측하는 FiLM-conditioned 네트워크로 구성된다. 시연 비디오 조건화 모드에서는 동일 과제의 다른 시연 비디오를 인코딩하여 조건으로 사용한다.",
    results: "학습 시 본 적 없는 과제에서 평균 약 44%의 제로샷 성공률을 달성했으며, 일부 간단한 과제에서는 90% 이상의 성공률을 보였다. 언어 조건화와 비디오 조건화 모두 효과적이었으며, 과제 수가 증가할수록 제로샷 일반화 성능이 향상되는 스케일링 경향을 확인했다.",
    impact: "BC-Z는 언어 조건화 로봇 정책의 제로샷 일반화 가능성을 대규모 실제 로봇 실험으로 처음 입증하여, 이후 RT-1, RT-2 등 대규모 로봇 학습 연구의 직접적인 선구자가 되었다. '더 많은 과제 데이터 → 더 나은 일반화'라는 스케일링 가설을 로봇 학습에서 실증하여, 범용 로봇 정책(generalist robot policy) 연구의 방향을 제시했다.",
    relatedFoundations: ["transformer"],
    relatedPapers: [
      { id: "rt1", fieldId: "robotics", title: "RT-1", relation: "successor" },
    ],
  },

  "cliport": {
    tldr: "CLIP의 시맨틱 이해 능력과 Transporter Network의 공간적 정밀도를 결합하여, 자연어 지시로 로봇 조작 과제를 수행하는 언어 조건화 모방 학습 프레임워크이다.",
    background: "로봇 조작에서 시맨틱 이해(어떤 물체를 어디에 놓을지)와 공간적 정밀도(정확한 픽 앤 플레이스 위치)는 모두 필수적이지만, 기존 방법들은 둘 중 하나에 치우치는 경향이 있었다. Transporter Network는 정밀한 공간적 추론에 뛰어나지만 시맨틱 이해가 부족했고, CLIP은 풍부한 시맨틱 표현을 제공하지만 픽셀 수준의 공간적 정밀도는 보장하지 못했다.",
    keyIdea: "CLIPort는 두 스트림 아키텍처를 통해 CLIP의 시맨틱 특징과 Transporter의 공간적 특징을 융합한다. Transporter Network의 파이프라인을 유지하되, 각 단계(pick과 place)에서 CLIP 비전 인코더의 다중 스케일 특징 맵을 추출하여 Transporter의 공간 스트림과 결합한다. 자연어 지시는 CLIP의 텍스트 인코더를 통해 인코딩되어 시맨틱 스트림에 조건으로 주입된다. 이를 통해 '빨간 블록을 녹색 그릇에 넣어'같은 언어 지시에 따라 정확한 위치를 추론할 수 있다.",
    method: "CLIP의 비전 인코더(ResNet-50)에서 다중 해상도 특징을 추출하고, 텍스트 인코더로 언어 지시를 인코딩한다. CLIP 특징을 Transporter의 attention과 transport 모듈에 주입하되, CLIP 가중치는 동결하고 융합 레이어만 학습한다. 시뮬레이션(Ravens 벤치마크)에서 10개 과제에 대해 실험하며, 과제당 1~1000개의 시연으로 학습한다.",
    results: "10개의 테이블탑 조작 과제에서 기존 Transporter Network과 언어 조건화 베이스라인을 크게 상회했다. 특히 소수 시연(1~10개)으로도 높은 성능을 보였으며, 학습 시 보지 못한 새로운 속성 조합(예: 새로운 색상-객체 조합)에 대한 일반화에서 CLIP 사전학습의 효과가 뚜렷했다.",
    impact: "CLIPort는 사전학습된 비전-언어 모델(VLM)을 로봇 조작에 활용하는 효과적인 방법을 제시하여, 파운데이션 모델과 로봇 학습의 연결 고리를 형성했다. 이후 SayCan, VIMA, PerAct 등 언어 조건화 로봇 조작 연구에 직접적 영향을 미쳤으며, 시맨틱 이해와 공간적 정밀도의 결합이라는 설계 원칙은 이후 많은 로봇 조작 시스템에서 채택되었다.",
    relatedFoundations: ["transformer", "vit"],
    relatedPapers: [
      { id: "saycan", fieldId: "robotics", title: "SayCan", relation: "successor" },
    ],
  },

  "rt1": {
    tldr: "130,000개의 실제 로봇 시연으로 학습한 대규모 Robotics Transformer 모델로, 단일 정책으로 700개 이상의 다양한 과제에서 97%의 성공률을 달성하며 실세계 로봇 학습의 새로운 패러다임을 제시했다.",
    background: "기존 로봇 학습 연구는 대부분 시뮬레이션이나 소규모 실제 데이터에 의존했으며, 과제 수가 제한적이었다. NLP와 컴퓨터 비전에서 데이터와 모델 규모의 확장이 극적인 성능 향상을 가져왔지만, 로봇 학습에서는 실제 데이터 수집의 높은 비용과 안전 제약으로 인해 유사한 스케일링이 시도되지 못했다. 다양한 과제를 단일 모델로 처리하면서도 실시간 추론이 가능한 아키텍처가 필요했다.",
    keyIdea: "RT-1(Robotics Transformer 1)은 '대규모 실제 로봇 데이터 + Transformer 아키텍처 = 범용 로봇 정책'이라는 스케일링 가설을 실증한다. 13대의 실제 로봇으로 17개월에 걸쳐 수집한 130K 에피소드를 사용하며, FiLM으로 언어 지시를 조건화한 EfficientNet 이미지 인코더와 TokenLearner로 압축된 시각 토큰을 Transformer에 입력하여 이산화된 행동 토큰을 출력한다. TokenLearner가 이미지 토큰 수를 크게 줄여 실시간(3Hz) 추론이 가능하며, 행동을 256개의 구간으로 이산화하여 분류 문제로 변환한다.",
    method: "6장의 이미지 히스토리와 자연어 지시를 입력으로 받아, EfficientNet-B3로 이미지 특징을 추출하고 FiLM 레이어로 언어 조건화한다. TokenLearner가 81개의 토큰을 8개로 압축한 후, Transformer 디코더가 7자유도 팔 행동(x, y, z, roll, pitch, yaw, gripper) + 베이스 이동(x, y, yaw) + 종료 모드를 이산 토큰으로 출력한다.",
    results: "700개 이상의 과제에서 97%의 성공률을 달성했으며, 학습 시 보지 못한 새로운 객체와 배경에 대해서도 76%의 일반화 성능을 보였다. SayCan과 결합하여 장기 과제 수행에서도 기존 시스템 대비 2배 이상의 성공률 향상을 달성했다. 데이터 다양성이 증가할수록 일반화 성능이 향상되는 스케일링 법칙을 확인했다.",
    impact: "RT-1은 대규모 실제 로봇 데이터를 활용한 범용 로봇 정책 학습이 실제로 가능함을 증명하여, 로봇 학습 분야의 패러다임 전환을 이끌었다. 이후 RT-2(VLM 활용), RT-X(다기관 데이터 통합), Open X-Embodiment 등 대규모 로봇 파운데이션 모델 연구의 출발점이 되었으며, Google DeepMind의 로봇 연구 방향을 결정짓는 핵심 성과가 되었다.",
    relatedFoundations: ["transformer", "vit"],
    relatedPapers: [
      { id: "rt2", fieldId: "robotics", title: "RT-2", relation: "successor" },
      { id: "saycan", fieldId: "robotics", title: "SayCan", relation: "related" },
    ],
  },

  "inner-monologue": {
    tldr: "LLM의 내적 독백(inner monologue)을 통해 로봇이 환경 피드백(성공/실패 감지, 객체 인식, 장면 설명)을 텍스트로 받아 재계획하며 체화된 추론(embodied reasoning)을 수행하는 프레임워크이다.",
    background: "SayCan 등의 LLM 기반 로봇 계획 시스템은 초기 계획을 생성한 후 실행하지만, 실행 중 발생하는 오류나 환경 변화에 대응하는 폐루프(closed-loop) 추론이 부족했다. 로봇이 실제 환경에서 동작할 때, 행동이 실패하거나 예상치 못한 상황이 발생하면 계획을 수정해야 하지만, 기존 시스템은 환경 피드백을 LLM에 전달하는 체계적 방법이 없었다.",
    keyIdea: "Inner Monologue는 로봇 실행 과정에서 다양한 환경 피드백 소스의 출력을 텍스트 형태로 LLM의 프롬프트에 지속적으로 주입하여, LLM이 '내적 독백'처럼 상황을 파악하고 계획을 동적으로 수정하게 한다. 피드백 소스는 세 가지로 구성된다: (1) 성공 감지기(행동의 성공/실패 판별), (2) 수동적 장면 묘사(로봇이 현재 보는 것에 대한 텍스트 설명), (3) 능동적 장면 묘사(특정 물체의 존재 여부 등에 대한 질의응답). 이러한 피드백이 텍스트로 변환되어 LLM의 대화 이력에 추가되면, LLM은 자연스럽게 맥락을 이해하고 다음 행동을 조정한다.",
    method: "사전학습된 LLM(PaLM)을 고수준 계획기로 사용하고, SayCan의 어포던스 기반 기술 선택을 저수준 실행기로 활용한다. 행동 실행 후 성공 감지기, 객체 감지기, 장면 설명 모델(VLM)의 출력을 텍스트화하여 LLM 프롬프트에 추가한다. 실제 로봇(Everyday Robots)과 시뮬레이션 환경(tabletop, kitchen) 모두에서 실험한다.",
    results: "실제 로봇 환경에서 피드백이 없는 개루프 계획 대비, Inner Monologue는 과제 성공률을 크게 향상시켰다. 특히 행동 실패 복구, 환경 변화 대응, 사용자의 실시간 지시 변경 등 동적 상황에서 효과가 뚜렷했다. 인간이 의도적으로 방해(물체 치우기 등)한 상황에서도 LLM이 피드백을 통해 대안 행동을 생성할 수 있었다.",
    impact: "Inner Monologue는 LLM 기반 로봇 시스템에서 폐루프 추론의 중요성을 확립하여, 이후 Voyager, Code as Policies, PROGPROMPT 등 환경 피드백을 활용하는 LLM-로봇 연구에 직접적 영향을 미쳤다. 다양한 감각 피드백을 텍스트로 통일하여 LLM에 전달한다는 아이디어는 멀티모달 에이전트 설계의 핵심 원칙으로 자리잡았다.",
    relatedFoundations: ["gpt3"],
    relatedPapers: [
      { id: "saycan", fieldId: "robotics", title: "SayCan", relation: "related" },
    ],
  },

  "gato": {
    tldr: "하나의 Transformer 모델이 텍스트, 이미지, 연속 제어를 포함한 604개의 서로 다른 과제를 수행하는 범용 에이전트(generalist agent)로, 게임 플레이, 대화, 이미지 캡셔닝, 로봇 조작을 단일 가중치로 처리한다.",
    background: "AI 연구는 오랫동안 개별 과제별 전문가 모델을 개발하는 방향으로 진행되어 왔다. NLP에서 GPT-3가 다양한 언어 과제를 하나의 모델로 처리할 수 있음을 보여주었지만, 언어 너머 시각, 로봇 제어, 게임 등 다양한 모달리티와 행동 공간을 단일 모델로 통합하는 것은 시도되지 않았다. 과연 하나의 네트워크가 근본적으로 다른 유형의 과제들을 동시에 학습할 수 있는지가 핵심 질문이었다.",
    keyIdea: "Gato는 모든 유형의 데이터를 토큰 시퀀스로 통일하여, 하나의 자기회귀 Transformer가 모든 과제를 시퀀스 예측 문제로 처리하게 한다. 텍스트는 SentencePiece 토큰으로, 이미지는 패치 단위 임베딩으로, 연속 값(로봇 관절 각도, 게임 보상 등)은 1024개 구간으로 이산화하여 토큰으로 변환한다. 각 에피소드의 (관측, 행동, 보상) 시퀀스를 토큰 시퀀스로 인코딩하고, 동일한 네트워크가 다음 토큰(행동)을 예측한다. 과제별 프롬프트나 조건화 없이, 순수하게 시퀀스 컨텍스트에서 과제를 추론한다.",
    method: "1.2B 파라미터의 Transformer 디코더를 사용하며, 604개 과제의 데이터를 혼합하여 학습한다. 과제 유형별로 별도의 토크나이저를 사용하되, 임베딩 이후에는 동일한 Transformer를 공유한다. 학습 데이터에는 Atari 게임, DM Control Suite, 실제 로봇 조작(시뮬레이션 및 실제), 이미지 캡셔닝, 대화 등이 포함된다. 컨텍스트 길이는 1024 토큰으로 제한한다.",
    results: "604개 과제 중 450개 이상에서 전문가 에이전트의 50% 이상 성능을 달성했다. Atari 게임에서는 인간 수준 이상의 게임이 다수 있었으며, 실제 로봇 블록 쌓기에서도 작동했다. 대화와 이미지 캡셔닝에서는 합리적인 품질을 보였으나, 전문가 모델에는 미치지 못했다.",
    impact: "Gato는 '하나의 모델이 모든 것을 할 수 있다'는 범용 에이전트(generalist agent)의 비전을 최초로 대규모로 실증했다. 비록 개별 과제에서 전문가 모델에 미치지 못하는 경우가 많았지만, 스케일링에 따른 성능 향상 가능성을 시사했다. Decision Transformer, RT-2 등 시퀀스 모델링 기반 의사결정 연구와 맥을 같이하며, 멀티모달 범용 에이전트 연구의 중요한 이정표가 되었다.",
    relatedFoundations: ["transformer"],
    relatedPapers: [
      { id: "decision-transformer", fieldId: "rl", title: "Decision Transformer", relation: "related" },
    ],
  },

  "voxposer": {
    tldr: "LLM과 VLM(비전-언어 모델)을 활용하여 3D 가치 맵(value map)과 비용 맵(cost map)을 생성하고, 이를 기반으로 로봇이 추가 학습 없이 제로샷으로 다양한 조작 과제를 수행하는 프레임워크이다.",
    background: "LLM 기반 로봇 시스템은 대부분 사전 정의된 기술(skill) 세트에 의존하여 고수준 계획을 생성하지만, 새로운 과제에 필요한 저수준 동작을 자동으로 생성하는 것은 여전히 어려웠다. '부드럽게 물건을 놓아라', '장애물을 피하면서 움직여라'와 같은 세밀한 동작 제약을 기존 기술 라이브러리로는 표현하기 어렵다. 언어 지시를 로봇의 3D 작업 공간에서의 연속적인 동작으로 직접 변환하는 방법이 필요했다.",
    keyIdea: "VoxPoser는 LLM이 코드를 생성하여 3D 복셀(voxel) 공간에 어포던스 맵(affordance map)과 제약 맵(constraint map)을 작성하게 한다. LLM은 자연어 지시를 파싱하여 '어디로 이동해야 하는지'(어트랙터)와 '어디를 피해야 하는지'(리펠러)를 3D 공간에 배치하는 파이썬 코드를 생성한다. VLM(예: OWL-ViT)이 객체의 3D 위치를 감지하고, 이를 코드에서 참조하여 가치 맵을 구성한다. 생성된 가치 맵은 모션 플래너의 목적함수로 직접 사용되어, 모델 예측 제어(MPC)가 로봇의 연속 궤적을 생성한다.",
    method: "GPT-4가 과제 지시를 입력받아, 복셀 공간에 가치를 할당하는 파이썬 코드를 생성한다. OWL-ViT와 깊이 카메라를 결합하여 객체의 3D 위치를 감지하고, 이를 코드에서 참조한다. 생성된 3D 가치 맵은 MPC(Model Predictive Control) 기반 모션 플래너의 비용 함수로 사용되며, 그리퍼의 개폐와 속도 조절도 LLM이 생성한 코드로 제어한다.",
    results: "실제 로봇에서 학습 데이터 없이 다양한 과제(물건 집기, 서랍 열기, 장애물 회피, 부드럽게 놓기 등)를 제로샷으로 수행했다. 기존 SayCan, Code as Policies 등과 비교하여 더 세밀한 동작 제어가 가능했으며, 특히 '빠르게', '조심스럽게' 등의 부사적 수식어에도 반응하여 동작 특성을 조절할 수 있었다.",
    impact: "VoxPoser는 LLM의 코드 생성 능력을 3D 공간 추론에 활용하여, 사전 학습된 기술 없이도 언어에서 저수준 로봇 동작을 직접 생성하는 새로운 접근법을 제시했다. 파운데이션 모델을 로봇의 연속 제어에 직접 연결하는 연구 흐름에서 중요한 진전을 이루었으며, 3D 가치 맵이라는 중간 표현의 효과성을 입증했다.",
    relatedFoundations: ["transformer", "gpt3"],
    relatedPapers: [
      { id: "saycan", fieldId: "robotics", title: "SayCan", relation: "prior" },
    ],
  },

  "aloha": {
    tldr: "저비용 양팔(bimanual) 원격 조작 하드웨어 시스템과 행동 청킹(Action Chunking with Transformers, ACT) 학습 알고리즘을 결합하여, 정밀한 양팔 조작 과제를 소수의 시연만으로 학습할 수 있는 로봇 시스템이다.",
    background: "정밀한 양팔 조작(예: 신발끈 묶기, 지퍼 올리기)은 로봇 학습의 오래된 난제이다. 고품질 양팔 시연 데이터 수집을 위한 원격 조작 시스템은 대부분 수십만 달러의 비용이 들어 접근성이 낮았다. 또한 행동 복제(behavioral cloning)는 축적되는 오류(compounding error) 문제로 정밀한 다단계 과제에서 성능이 급격히 저하되는 한계가 있었다.",
    keyIdea: "ALOHA의 핵심은 하드웨어와 알고리즘의 공동 혁신에 있다. 하드웨어 측면에서, 각 2만 달러 미만의 저비용 ViperX 로봇 팔 4개(리더 2개 + 팔로워 2개)로 구성된 양팔 원격 조작 시스템을 설계한다. 알고리즘 측면에서, ACT(Action Chunking with Transformers)는 단일 타임스텝의 행동 대신 미래 k 스텝의 행동 시퀀스(청크)를 한 번에 예측하는 CVAE(Conditional VAE) + Transformer 구조를 사용한다. 행동 청킹은 각 예측 간의 시간적 일관성을 보장하고, 축적 오류를 효과적으로 줄인다. CVAE의 스타일 변수는 시연의 다양한 전략을 포착하여 모드 평균화(mode averaging) 문제를 완화한다.",
    method: "리더 로봇을 인간이 직접 조작하면 팔로워 로봇이 동일한 동작을 실시간으로 모방하며 시연을 기록한다. ACT는 관절 위치와 카메라 이미지를 입력으로 받아 CVAE 인코더가 스타일 변수 z를 생성하고, Transformer 디코더가 z에 조건화된 k-스텝 행동 청크를 출력한다. 과제당 50회 이내의 시연으로 학습하며, temporal ensembling으로 연속된 청크 간의 전환을 부드럽게 한다.",
    results: "핀 삽입(PIN insertion), 케이블 라우팅, 링 스태킹 등 정밀한 양팔 조작 과제에서 80~96%의 성공률을 달성했다. 단순 행동 복제(MLP/Transformer) 대비 ACT가 일관되게 우수했으며, 행동 청킹과 CVAE가 각각 독립적으로 성능 향상에 기여함을 확인했다.",
    impact: "ALOHA는 저비용 양팔 원격 조작과 효과적인 모방 학습의 조합으로 정밀 로봇 조작 연구의 접근성을 혁명적으로 높였다. 오픈소스 하드웨어 설계와 코드 공개로 전 세계 연구실에서 재현되었으며, Mobile ALOHA, ALOHA 2 등의 후속 연구와 상용화(Google DeepMind)로 이어졌다. 행동 청킹은 로봇 모방 학습의 표준 기법으로 자리잡았다.",
    relatedFoundations: ["transformer"],
    relatedPapers: [
      { id: "mobile-aloha", fieldId: "robotics", title: "Mobile ALOHA", relation: "successor" },
    ],
  },

  "mobile-aloha": {
    tldr: "ALOHA 양팔 시스템에 이동 가능한 베이스를 결합하고, 기존 정적 ALOHA 데이터와의 공동 학습(co-training)을 통해 소수의 시연만으로 전신 이동 조작 과제를 학습할 수 있는 모바일 양팔 로봇 시스템이다.",
    background: "ALOHA는 정밀한 양팔 조작에서 뛰어난 성과를 보였지만, 고정된 베이스로 인해 작업 범위가 제한적이었다. 실제 가정과 사무 환경에서는 방 사이를 이동하면서 물건을 나르거나, 걸어가서 서랍을 열고 물건을 꺼내는 등 이동과 조작이 결합된 과제가 대부분이다. 그러나 모바일 조작은 이동과 양팔 조작을 동시에 제어해야 하므로 행동 공간이 크고, 충분한 시연 데이터 수집이 어렵다는 과제가 있었다.",
    keyIdea: "Mobile ALOHA의 핵심 기여는 두 가지이다. 첫째, 하드웨어 측면에서 ALOHA 양팔 시스템을 AgileX 이동 베이스 위에 장착하고, 인간이 베이스를 밀면서 양팔을 원격 조작하는 전신 시연 수집 시스템을 구축했다. 둘째, 그리고 더 중요한 알고리즘 기여로, 소수(약 50회)의 모바일 시연만으로는 학습이 어려운 문제를 기존 정적 ALOHA 데이터(수백~수천 에피소드)와의 공동 학습(co-training)으로 해결한다. 정적 데이터가 양팔 조작의 기본 능력을 제공하면, 소수의 모바일 데이터로부터 이동 + 조작의 통합 정책을 효과적으로 학습할 수 있다.",
    method: "베이스의 선속도와 각속도(2차원)를 양팔 관절(14차원)에 추가하여 총 16차원의 행동 공간을 구성한다. ACT(Action Chunking with Transformers)를 정책 아키텍처로 사용하며, 정적 ALOHA 데이터셋과 모바일 데이터셋을 50:50 비율로 혼합하여 학습한다. 정적 데이터의 베이스 행동은 0으로 패딩한다.",
    results: "5가지 모바일 조작 과제(캐비닛에서 냄비 꺼내기, 전자레인지 사용하기, 엘리베이터 타기 등)에서 공동 학습이 과제당 50회 시연만으로 평균 80% 이상의 성공률을 달성했다. 공동 학습 없이 모바일 데이터만으로 학습하면 성공률이 크게 하락(20~50%p)하여, 공동 학습의 효과가 뚜렷했다.",
    impact: "Mobile ALOHA는 이동 가능한 양팔 로봇이 가정 환경의 복잡한 과제를 수행할 수 있음을 실증하여 큰 대중적 관심을 받았다(요리, 청소 시연 영상이 바이럴). 공동 학습이라는 간단하면서도 효과적인 기법은 데이터 부족 문제를 해결하는 범용적 접근법으로 인정받았으며, 구글 DeepMind의 ALOHA 2 상용화와 다수의 후속 연구에 영향을 미쳤다.",
    relatedFoundations: ["transformer"],
    relatedPapers: [
      { id: "aloha", fieldId: "robotics", title: "ALOHA", relation: "prior" },
      { id: "rt2", fieldId: "robotics", title: "RT-2", relation: "related" },
    ],
  },

  // ===== World Models Field =====
  "dreamerv1": {
    tldr: "잠재 공간(latent space)에서 환경 역학 모델을 학습하고, 이 모델의 상상(imagination) 속에서 액터-크리틱을 학습하는 모델 기반 강화학습 알고리즘으로, 잠재 상상 속의 역전파를 통해 장기 행동을 효율적으로 학습한다.",
    background: "모델 기반 강화학습은 환경 모델을 학습하여 샘플 효율을 높이는 접근법이지만, 이미지와 같은 고차원 관측 공간에서의 세계 모델 학습은 어렵고, 학습된 모델의 예측 오류가 누적되어 장기 계획에 한계가 있었다. World Models(Ha & Schmidhuber, 2018)가 잠재 공간에서의 환경 모델링을 제안했지만, 정책 학습이 진화 전략에 의존하여 확장성이 제한적이었다.",
    keyIdea: "DreamerV1(Dreamer)은 RSSM(Recurrent State-Space Model)을 세계 모델로 사용하여 잠재 역학을 학습한 후, 이 모델 내에서 완전히 상상으로만 경험을 생성하고 액터-크리틱을 학습한다. 핵심 혁신은 상상된 궤적에 대해 역전파(backpropagation through imagination)를 수행하여 정책을 학습한다는 것이다. RSSM은 결정론적 경로(GRU)와 확률론적 상태(가우시안 잠재 변수)를 결합하여, 부분 관측 환경에서도 효과적으로 상태를 추론한다. 세계 모델은 관측 재구성, 보상 예측, 할인 예측의 세 가지 손실로 학습되며, 정책은 모델 내에서 시뮬레이션된 경험의 가치를 최대화하도록 학습된다.",
    method: "학습은 세 단계를 반복한다: (1) 실제 환경과 상호작용하여 경험을 리플레이 버퍼에 저장, (2) 버퍼에서 시퀀스를 샘플링하여 RSSM 세계 모델 학습, (3) 학습된 모델의 잠재 공간에서 상상 롤아웃을 생성하고 액터-크리틱 학습. 액터는 상상된 가치의 그래디언트를 역전파하여 학습하며, 크리틱은 λ-리턴을 타겟으로 학습한다.",
    results: "DeepMind Control Suite 20개 과제에서 기존 모델 기반(PlaNet, SLAC) 및 모델 프리(D4PG, A3C) 알고리즘을 상회했다. 특히 샘플 효율에서 모델 프리 방법 대비 20배 이상 적은 상호작용으로 동등한 성능을 달성했으며, 이미지 입력에서도 안정적으로 작동했다.",
    impact: "DreamerV1은 잠재 상상 속 정책 학습이라는 체계적 프레임워크를 확립하여, 이후 DreamerV2(이산 잠재 표현, Atari 마스터), DreamerV3(범용 하이퍼파라미터) 시리즈의 기반이 되었다. RSSM은 세계 모델의 표준 아키텍처로 자리잡았으며, 모델 기반 RL이 이미지 기반 연속 제어에서 실용적으로 적용 가능함을 증명한 전환점이 되었다.",
    relatedFoundations: ["vae", "lstm"],
    relatedPapers: [
      { id: "world-models-ha", fieldId: "world-models", title: "World Models", relation: "prior" },
      { id: "dreamerv2", fieldId: "world-models", title: "DreamerV2", relation: "successor" },
    ],
  },

  "dreamerv2": {
    tldr: "이산 잠재 표현(categorical latent)을 도입한 세계 모델과 상상 속 액터-크리틱 학습을 결합하여, 모델 기반 강화학습으로서 최초로 Atari 벤치마크에서 모델 프리 알고리즘(Rainbow)의 성능에 필적하는 성과를 달성했다.",
    background: "DreamerV1은 연속 제어(DMC)에서 뛰어난 성과를 보였지만, Atari와 같은 이산 행동 공간과 시각적으로 복잡한 환경에서는 모델 프리 알고리즘에 미치지 못했다. 특히 Atari 55개 게임 벤치마크에서 모델 기반 RL이 모델 프리의 최고 수준(Rainbow DQN)에 도달하지 못하는 것은 해당 분야의 장기적 과제였다. 가우시안 잠재 공간의 표현력과 이산 행동 공간에서의 정책 학습의 한계가 주요 병목이었다.",
    keyIdea: "DreamerV2의 핵심 혁신은 세계 모델의 잠재 상태를 가우시안에서 범주형 분포(categorical distribution)로 전환한 것이다. 32개의 범주형 변수, 각각 32개 클래스를 가지는 잠재 상태는 총 32^32의 조합 가능성을 제공하여 복잡한 환경을 효과적으로 표현한다. 이산 잠재 변수의 그래디언트는 straight-through 추정기로 전파한다. 추가적으로 KL 밸런싱이라는 기법을 도입하여 사전(prior)과 사후(posterior) 분포 간의 KL 발산 학습 속도를 비대칭적으로 조절함으로써, 세계 모델이 더 정보가 풍부한 잠재 표현을 학습하도록 유도한다.",
    method: "RSSM의 확률론적 부분을 32x32 범주형 분포로 대체하고, straight-through 그래디언트로 역전파한다. KL 손실을 α:β = 0.8:0.2로 비대칭 분배하여 사후분포가 사전분포를 따라가도록 유도한다. 상상 속에서 Reinforce + straight-through 혼합 그래디언트로 액터를 학습하고, 이산 행동 공간에서는 straight-through을 통한 원-핫 행동 샘플링을 사용한다.",
    results: "Atari 55개 게임에서 인간 정규화 중앙값 점수 209.2%를 달성하여, 모델 프리 알고리즘 Rainbow(223%)에 근접하며 IQN(218%)과 동등한 수준에 도달했다. 이는 모델 기반 RL이 Atari에서 모델 프리와 경쟁적 성능을 달성한 최초의 사례이다. 동시에 DMC 연속 제어 벤치마크에서도 DreamerV1을 상회했다.",
    impact: "DreamerV2는 모델 기반 RL의 가능성에 대한 인식을 근본적으로 전환하여, '모델 기반은 모델 프리에 미치지 못한다'는 통념을 깨뜨렸다. 이산 잠재 표현은 이후 DreamerV3, IRIS 등에서 채택되었으며, 범주형 세계 모델은 비디오 예측과 세계 시뮬레이터 연구에도 영향을 미쳤다.",
    relatedFoundations: ["vae"],
    relatedPapers: [
      { id: "dreamerv1", fieldId: "world-models", title: "DreamerV1", relation: "prior" },
      { id: "iris", fieldId: "world-models", title: "IRIS", relation: "successor" },
    ],
  },

  "jepa": {
    tldr: "Yann LeCun이 제안한 자율 기계 지능(autonomous machine intelligence)의 청사진으로, 공동 임베딩 예측 아키텍처(Joint Embedding Predictive Architecture, JEPA)를 중심으로 세계 모델 기반의 계획, 계층적 추론, 에너지 기반 학습을 통합하는 프레임워크를 제시했다.",
    background: "현재 AI 시스템은 방대한 데이터로 학습해도 인간과 동물의 상식적 세계 이해와 효율적 학습에 크게 미치지 못한다. 생성 모델은 입력 공간에서 직접 예측하므로 불필요한 세부사항까지 모델링해야 하고, 대조 학습은 부정 샘플 의존성이 높다. 자기지도 학습이 강력한 표현을 학습할 수 있음이 밝혀졌지만, 이를 행동, 계획, 추론으로 확장하는 체계적 프레임워크가 부재했다.",
    keyIdea: "JEPA의 핵심은 입력 공간이 아닌 표현 공간에서 예측을 수행하는 것이다. 이미지의 한 부분으로부터 다른 부분의 표현을 예측하되, 픽셀을 직접 재구성하지 않고 추상적 특징 벡터를 예측한다. 이를 통해 예측에 불필요한 세부사항을 자연스럽게 제거할 수 있다. 더 넓은 프레임워크에서, LeCun은 세계 모델, 액터, 인지기(perceiver), 단기 기억, 비용 모듈이 상호작용하는 인지 아키텍처를 제안한다. 세계 모델이 행동의 결과를 내부적으로 시뮬레이션하고, 그래디언트 기반 계획을 통해 최적 행동 시퀀스를 탐색한다. 에너지 기반 모델(EBM) 프레임워크에서 정규화(regularization)를 통해 표현 공간의 붕괴(collapse)를 방지한다.",
    method: "I-JEPA(Image-JEPA)를 구체적 구현으로 제시한다. ViT 기반 인코더가 이미지의 컨텍스트 블록을 인코딩하고, 예측기가 마스킹된 타겟 블록의 표현을 예측한다. 타겟 인코더는 EMA(Exponential Moving Average)로 업데이트되어 안정적인 타겟을 제공한다. 생성적 디코딩이나 부정 샘플 없이, 순수하게 표현 공간의 예측 손실만으로 학습한다.",
    results: "I-JEPA는 ImageNet 선형 평가에서 MAE와 data2vec을 상회하면서, 학습 효율이 크게 높았다(MAE의 2.5배 빠른 수렴). 의미론적 과제(분류)와 저수준 과제(객체 카운팅) 모두에서 우수한 성능을 보였으며, 생성 모델이나 불변 기반 방법과 달리 두 유형의 과제를 동시에 잘 수행했다.",
    impact: "JEPA 논문은 차세대 AI 아키텍처에 대한 가장 영향력 있는 비전 중 하나로, 세계 모델 기반 자율 지능에 대한 연구 방향을 제시했다. '표현 공간에서의 예측'이라는 원칙은 V-JEPA(비디오), A-JEPA(오디오) 등으로 확장되었으며, Meta의 AI 연구 전략에 직접적 영향을 미쳤다. 생성 모델(LLM, 디퓨전) 중심의 현재 AI 패러다임에 대한 근본적 대안을 제시했다는 점에서 학술적 의의가 크다.",
    relatedFoundations: ["vae", "backpropagation"],
    relatedPapers: [
      { id: "byol", fieldId: "representation", title: "BYOL", relation: "related" },
    ],
  },

  "iris": {
    tldr: "관측을 이산 토큰으로 변환한 뒤 Transformer 세계 모델이 토큰 공간에서 다음 관측, 보상, 종료를 자기회귀적으로 예측하여 상상 속에서 정책을 학습하는 모델 기반 강화학습 알고리즘이다.",
    background: "기존 세계 모델(Dreamer 시리즈)은 연속 잠재 공간이나 범주형 잠재 공간에서 RNN 기반 시퀀스 모델로 역학을 학습했다. 한편 NLP에서 Transformer의 자기회귀 생성이 놀라운 성공을 거두었고, 이미지 분야에서도 VQ-VAE/VQGAN이 이미지를 이산 토큰으로 변환하는 효과적인 방법을 제공했다. Transformer의 강력한 시퀀스 모델링 능력을 세계 모델에 직접 활용하려는 시도가 자연스럽게 등장했다.",
    keyIdea: "IRIS(Imagination with auto-Regression over an Inner Speech)는 두 단계 아키텍처로 구성된다. 첫째, VQ-VAE(이산 오토인코더)가 이미지 관측을 이산 토큰 시퀀스로 인코딩한다. 둘째, GPT 스타일의 자기회귀 Transformer가 (관측 토큰, 행동, 보상, 종료) 시퀀스를 다음 토큰 예측으로 학습한다. 이 세계 모델은 현재 상태와 행동이 주어지면 다음 관측 토큰, 보상, 종료 여부를 자기회귀적으로 생성하여 완전한 상상 궤적을 만들어낸다. 이 상상 속에서 Actor-Critic을 학습하며, 실제 환경과의 상호작용 없이도 정책을 개선한다.",
    method: "16x16 이미지 패치를 VQ-VAE로 인코딩하여 각 관측을 16개의 이산 토큰으로 변환한다. Transformer 세계 모델은 과거 맥락(관측 토큰 + 행동)에서 다음 관측 토큰, 보상, 종료를 순차적으로 예측한다. 환경에서 수집한 실제 경험과 세계 모델이 생성한 상상 경험을 모두 활용하여 Actor-Critic을 학습한다.",
    results: "Atari 100K 벤치마크(환경 상호작용 100K 스텝 제한)에서 26개 게임 중 10개에서 인간 성능을 초과했으며, 평균 인간 정규화 점수에서 DreamerV2, SPR 등 기존 방법을 상회했다. 특히 시각적으로 복잡한 게임(Breakout, Asterix 등)에서 뛰어난 성능을 보여, 이산 토큰 기반 세계 모델의 효과를 입증했다.",
    impact: "IRIS는 Transformer 기반 자기회귀 세계 모델이 RL에서 효과적임을 증명하여, 이후 DIAMOND, Genie 등 토큰 기반 세계 모델 연구의 흐름을 촉발했다. NLP의 대규모 언어 모델링 패러다임을 세계 모델링에 직접 적용할 수 있다는 가능성을 열었으며, 비디오 생성 모델과 세계 모델의 융합 연구에도 영향을 미쳤다.",
    relatedFoundations: ["transformer", "vit"],
    relatedPapers: [
      { id: "dreamerv2", fieldId: "world-models", title: "DreamerV2", relation: "prior" },
      { id: "genie", fieldId: "world-models", title: "Genie", relation: "related" },
    ],
  },

  "unisim": {
    tldr: "인터넷 비디오, 로봇 데이터, 시뮬레이터 데이터를 통합 학습하여 다양한 행동에 반응하는 상호작용형 실세계 시뮬레이터를 구축하고, 이를 통해 로봇 정책을 현실 전이(sim-to-real) 없이 학습할 수 있음을 보여준 범용 시뮬레이터 모델이다.",
    background: "로봇 정책 학습은 대규모 실세계 상호작용 데이터의 부족이 핵심 병목이다. 물리 시뮬레이터(MuJoCo, Isaac Sim)는 현실 격차(reality gap) 문제가 있고, 인터넷 비디오는 대규모이지만 행동 레이블이 없으며 상호작용적이지 않다. 다양한 데이터 소스를 결합하여 상호작용 가능한 실세계 시뮬레이터를 구축할 수 있다면, 로봇 학습의 데이터 부족 문제를 근본적으로 해결할 수 있을 것이라는 비전이 있었다.",
    keyIdea: "UniSim은 '행동'의 개념을 통일하여 다양한 수준의 제어 신호를 하나의 모델로 처리한다. 고수준 행동(자연어 설명: '컵을 왼쪽으로 밀어'), 중수준 행동(카메라 이동), 저수준 행동(로봇 엔드이펙터 궤적)을 모두 하나의 비디오 생성 모델의 조건으로 사용한다. 인터넷 비디오에서는 언어 조건화, 로봇 데이터에서는 행동 조건화, 시뮬레이터 데이터에서는 물리적 상호작용을 학습하여, 이들을 결합하면 언어 지시에 반응하면서도 물리적으로 그럴듯한 비디오 시뮬레이션을 생성할 수 있다.",
    method: "비디오 디퓨전 모델을 기반으로 하며, 다양한 행동 유형을 조건으로 받아 다음 비디오 프레임을 생성한다. 인터넷 비디오(텍스트 캡션), 로봇 조작 데이터(엔드이펙터 궤적), 인터넷 네비게이션 데이터(방향 명령)를 통합 학습한다. 학습된 시뮬레이터에서 강화학습(PPO)이나 모방 학습으로 정책을 학습하고, 이를 실제 로봇에 직접 전이한다.",
    results: "학습된 시뮬레이터에서 훈련한 로봇 정책이 실제 환경에 zero-shot으로 전이되어 작동했다. 장기 비디오 생성에서도 시각적 일관성을 유지했으며, 다양한 행동 유형(언어, 궤적, 방향)에 대해 그럴듯한 상호작용 영상을 생성했다. 텍스트 조건 비디오 생성에서는 기존 비디오 생성 모델과 경쟁적 품질을 보였다.",
    impact: "UniSim은 '학습된 범용 시뮬레이터'라는 비전을 구체화하여, Sora, Genie 등 세계 시뮬레이터 연구의 흐름에서 로봇 학습에의 직접적 적용 가능성을 보여주었다. 인터넷 비디오와 로봇 데이터의 통합 학습이라는 접근법은 데이터 스케일링의 새로운 방향을 제시했으며, 물리 시뮬레이터를 학습된 모델로 대체하는 패러다임 전환을 시사했다.",
    relatedFoundations: ["transformer", "ddpm"],
    relatedPapers: [
      { id: "sora", fieldId: "world-models", title: "Sora", relation: "related" },
    ],
  },

  "gamenngen": {
    tldr: "디퓨전 모델이 게임 엔진을 대체하여 클래식 FPS 게임 DOOM을 실시간으로 인터랙티브하게 시뮬레이션하는 데 최초로 성공한 뉴럴 게임 엔진으로, 게임 코드 없이 플레이어 입력에 반응하는 프레임을 초당 20프레임 이상으로 생성한다.",
    background: "게임 엔진은 전통적으로 수작업 물리 시뮬레이션, 렌더링 파이프라인, 게임 로직 코드로 구현된다. 한편 비디오 생성 모델의 발전으로 사실적인 영상 생성이 가능해졌지만, 실시간 상호작용과 긴 시간 동안의 일관성을 유지하는 것은 여전히 어려웠다. 뉴럴 네트워크가 게임의 물리, 렌더링, 로직을 모두 내재화하여 완전한 게임 경험을 제공할 수 있는지가 근본적 질문이었다.",
    keyIdea: "GameNGen은 두 단계로 구성된다. 첫째, RL 에이전트(PPO로 학습)가 DOOM을 플레이하며 대규모 게임 세션 데이터(행동-프레임 시퀀스)를 생성한다. 둘째, 안정적 디퓨전(Stable Diffusion) 기반 모델을 이 데이터로 미세조정하여, 이전 프레임들과 플레이어 행동을 조건으로 다음 프레임을 생성하도록 학습한다. 핵심 기술적 혁신은 노이즈 증강(noise augmentation)으로, 학습 시 조건 프레임에 다양한 수준의 노이즈를 추가하여 자기회귀 생성 시의 오류 누적을 방지한다. 이를 통해 수 분간의 안정적인 실시간 게임 플레이가 가능해진다.",
    method: "PPO로 학습된 에이전트가 DOOM을 플레이하며 9억 프레임의 학습 데이터를 생성한다. Stable Diffusion 1.4를 기반으로, U-Net의 입력에 이전 프레임(최대 64프레임 히스토리)과 행동을 조건으로 주입한다. 학습 시 조건 프레임에 무작위 노이즈를 추가하되, 노이즈 수준도 U-Net에 입력하여 노이즈 수준을 인식하게 한다. 추론 시에는 3.4 디노이징 스텝만으로 단일 TPU v5에서 20FPS 이상을 달성한다.",
    results: "인간 평가자가 실제 DOOM 게임 클립과 GameNGen이 생성한 클립을 구분하지 못하는 수준의 시각적 품질을 달성했다(짧은 클립에서 58.4%의 정확도로 사실상 랜덤 수준). LPIPS 메트릭에서 JPEG과 유사한 수준의 프레임 품질을 보였다. 에이전트가 적을 쏘고, 문을 열고, 아이템을 줍는 등 게임의 핵심 메카닉이 정확히 시뮬레이션되었다.",
    impact: "GameNGen은 뉴럴 네트워크가 완전한 인터랙티브 게임 엔진으로 기능할 수 있음을 최초로 증명하여, 게임 개발과 시뮬레이션의 미래에 대한 근본적 질문을 제기했다. Sora, Genie와 함께 인터랙티브 세계 시뮬레이션이라는 새로운 연구 분야를 형성하고 있으며, 코드 없는 게임 엔진, 학습 기반 물리 시뮬레이션, 그리고 범용 세계 모델 연구에 중요한 이정표가 되었다.",
    relatedFoundations: ["ddpm", "transformer"],
    relatedPapers: [
      { id: "sora", fieldId: "world-models", title: "Sora", relation: "related" },
      { id: "genie", fieldId: "world-models", title: "Genie", relation: "related" },
    ],
  },
};
